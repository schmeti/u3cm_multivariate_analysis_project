{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 2\n",
    "------------------------------------------------------------------------\n",
    "Laura Silvana Alvarez, Florencia Luque and Simon Schmetz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kmeans\n",
    "\n",
    "In the first step, we will compare different types of clustering using the K-Means algorithm. The first approach focuses solely on numeric variables, comparing two scenarios: one using the raw data (excluding variables with high correlations) and another using the PCA components derived in the first part of the analysis.\n",
    "\n",
    "For the second approach, K-Means will be applied to mixed data, combining both numeric and categorical variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kmeans with PCA data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "### import libraries\n",
    "\n",
    "# format\n",
    "import pandas as pd\n",
    "\n",
    "# plot\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# math\n",
    "import numpy as np\n",
    "from scipy.linalg import inv, det\n",
    "from numpy.linalg import eig\n",
    "\n",
    "#kmeans \n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "#kmeans mixto\n",
    "from kmodes.kprototypes import KPrototypes\n",
    "from dython.nominal import associations\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "#PCA\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler,LabelEncoder\n",
    "\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to create summary of kmeans\n",
    "def summarize_clusters(df,cluster_column, numeric_cols, categorical_cols, binary_col):\n",
    "    cluster_summary = []\n",
    "    \n",
    "    for cluster in sorted(df[cluster_column].unique()):\n",
    "        cluster_data = df[df[cluster_column] == cluster]\n",
    "        summary = {}\n",
    "        \n",
    "        # mean for numeric variables\n",
    "        for col in numeric_cols:\n",
    "            summary[col] = cluster_data[col].mean()\n",
    "        \n",
    "        # mode for categorical variables\n",
    "        for col in categorical_cols:\n",
    "            summary[col] = cluster_data[col].mode()[0]  # mode()[0] gives the most frequent value\n",
    "        \n",
    "        # percentage of 1's for the binary column\n",
    "        total_count = len(cluster_data)\n",
    "        count_ones = cluster_data[binary_col].sum()\n",
    "        percentage_ones = (count_ones / total_count) * 100\n",
    "        summary[f'{binary_col}_percentage'] = round(percentage_ones, 2)\n",
    "        \n",
    "        # Add cluster label\n",
    "        summary[cluster_column] = cluster\n",
    "        cluster_summary.append(summary)\n",
    "    \n",
    "    # Convert summary list to a DataFrame\n",
    "    summary_df = pd.DataFrame(cluster_summary)\n",
    "    return summary_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scatterplot_pc(df, x_col, y_col, hue_col=None, ax=None, expl_var=None):\n",
    "    # plot\n",
    "    sns.scatterplot(data=df, \n",
    "                    x=x_col, \n",
    "                    y=y_col, \n",
    "                    hue=hue_col,\n",
    "                    palette='Set1',\n",
    "                    ax=ax)\n",
    "    \n",
    "    # cosmetics\n",
    "    title = f'Scatter Plot of {y_col} vs {x_col}'\n",
    "    if expl_var is not None:\n",
    "        title += f' \\n Joined explained variance: {expl_var} %'\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(x_col)\n",
    "    ax.set_ylabel(y_col)\n",
    "    ax.axhline(0, color='gray', linestyle='--', linewidth=0.7)\n",
    "    ax.axvline(0, color='gray', linestyle='--', linewidth=0.7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "data_pca = pd.read_csv(\"data_pca.csv\")\n",
    "data_pca = data_pca.drop(columns=[\"Unnamed: 0\",\"PC4\",\"PC5\",\"PC6\",\"PC7\",\"index\"])\n",
    "data_raw = pd.read_csv(\"Sleep_health_and_lifestyle_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Preprocess Data\n",
    "data = data_raw.copy()\n",
    "\n",
    "# rename columns for easier use\n",
    "rename_dict = {\n",
    "    'Person ID':'person_id',\n",
    "    'Gender': 'gender',\n",
    "    'Age':'age',\n",
    "    'Occupation':'occupation',\n",
    "    'Sleep Duration':'sleep_duration',\n",
    "    'Quality of Sleep':'quality_of_sleep',\n",
    "    'Physical Activity Level':'physical_activity_level',\n",
    "    'Stress Level':'stress_level',\n",
    "    'BMI Category':'bmi_category', \n",
    "    'Blood Pressure':'blood_pressure', \n",
    "    'Heart Rate':'heart_rate', \n",
    "    'Daily Steps':'daily_steps',\n",
    "    'Sleep Disorder':'sleep_disorder' \n",
    "}\n",
    "data.rename(columns=rename_dict, inplace=True)\n",
    "\n",
    "# change dtype\n",
    "data['quality_of_sleep'] = data['quality_of_sleep'].astype(str)\n",
    "data['stress_level'] = data['stress_level'].astype(str)\n",
    "\n",
    "# make sleep disorder binary\n",
    "data['sleep_disorder'] = data['sleep_disorder'].map(lambda x: '1' if x in ['Insomnia','Sleep Apnea'] else '0').astype(str)\n",
    "\n",
    "# split blood pressure into diastolic & systolic\n",
    "data[[\"blood_pressure_systolic\",\"blood_pressure_diastolic\"]] = data[\"blood_pressure\"].str.split('/',expand=True)\n",
    "data[\"blood_pressure_diastolic\"] = pd.to_numeric(data['blood_pressure_diastolic'])\n",
    "data[\"blood_pressure_systolic\"] = pd.to_numeric(data[\"blood_pressure_systolic\"])\n",
    "\n",
    "#data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # set up column lists\n",
    "numeric_variables = ['age','sleep_duration','physical_activity_level','heart_rate','daily_steps','blood_pressure_systolic','blood_pressure_diastolic']\n",
    "categorical_variables = ['gender','occupation','quality_of_sleep','stress_level','bmi_category','sleep_disorder']\n",
    "# data.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pca = data_pca[['PC1','PC2','PC3']]\n",
    "silhouette_scores = []\n",
    "inertia = []\n",
    "for k in range(2, 11):  # Silhouette score is undefined for k=1\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    kmeans.fit(X_pca)\n",
    "    score = silhouette_score(X_pca, kmeans.labels_)\n",
    "    silhouette_scores.append(score)\n",
    "    inertia.append(kmeans.inertia_)\n",
    "# Plotting the Silhouette Scores\n",
    "fig, axs = plt.subplots(1,2,figsize=(6,2))\n",
    "axs[0].plot(range(2, 11), silhouette_scores)\n",
    "axs[0].set_title('Silhouette Method for Optimal k')\n",
    "axs[0].set_xlabel('Number of Clusters (k)')\n",
    "axs[0].set_ylabel('Silhouette Score')\n",
    "axs[1].plot(range(2, 11), inertia)\n",
    "axs[1].set_title('Elbow Method for Optimal k')\n",
    "axs[1].set_xlabel('Number of Clusters (k)')\n",
    "axs[1].set_ylabel('WCSS')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we compare the graphs, there is stabilization between 5 and 7 clusters in the Silhouette method. Similarly, in the Elbow method, the slope becomes less steep around 5 and 7 clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 2: Perform K-Means clustering on the reduced data\n",
    "#kmeans = KMeans(n_clusters=5, random_state=14, init=\"random\", n_init=30)\n",
    "#kmeans.fit(X_pca)\n",
    "\n",
    "# Get the cluster labels\n",
    "#labels = kmeans.labels_\n",
    "\n",
    "# Checking clusters (silently store metrics without printing)\n",
    "#inertia = kmeans.inertia_\n",
    "#sil_score = silhouette_score(X_pca, kmeans.labels_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Between the cluster 3 and 1 is some elapsing data within the PC1 vs PC2. This also ocurre within the 1 and the 0 in the PC1 vs PC3. The other clusters all well separarted in the first factorial plane."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Step 2: Perform K-Means clustering on the reduced data\n",
    "kmeans = KMeans(n_clusters=5, random_state=14, init=\"random\", n_init=30)\n",
    "kmeans.fit(X_pca)\n",
    "\n",
    "# Get the cluster labels\n",
    "labels = kmeans.labels_\n",
    "data_pca['Cluster'] = kmeans.labels_\n",
    "# Step 3: Visualize the results\n",
    "#cmap='viridis'\n",
    "\n",
    "fig, axs = plt.subplots(1, 3, figsize=(8, 3))\n",
    "scatterplot_pc(data_pca,x_col=\"PC1\",y_col = \"PC2\", hue_col=labels,ax = axs[0])\n",
    "scatterplot_pc(data_pca,x_col=\"PC1\",y_col = \"PC3\", hue_col=labels,ax = axs[1])\n",
    "scatterplot_pc(data_pca,x_col=\"PC2\",y_col = \"PC3\", hue_col=labels,ax = axs[2])\n",
    "plt.show()\n",
    "\n",
    "# CHECKING THE CLUSTERS\n",
    "# Get the inertia (WCSS)\n",
    "inertia = kmeans.inertia_\n",
    "print(f\"Inertia (WCSS): {inertia}\")\n",
    "# Assuming kmeans.labels_ contains the cluster labels\n",
    "sil_score = silhouette_score(X_pca, kmeans.labels_)\n",
    "print(f\"Silhouette Score: {sil_score}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we use 5 cluster there's seem to be more clarity between the clusters. The silhouette is also almost over 0.5 and it visible in the graphs. \n",
    "\n",
    "If we compare the information we can said that the best number of cluster for the PCA data is 7, but we decide to keep 5, because it does not make sense to have that amount of clusters when the sample is not that large.\n",
    "\n",
    "The principal characteristics of the clusters are given in the next table. Where in the categorical variables we have the mode of the cluster, and in the end we have the proportion of the cluster that has sleep disorders.\n",
    "\n",
    "- In cluster 2 and 4 it is shown that there are more nurse females with overweight. Despite the fact that the cluster 0 has a good perception of their quality of sleep, it is shown that both clusters have the biggest % sleep disorder (over 90%)\n",
    "\n",
    "- Cluster 3 have the lowest proportions of sleep disorder, most of the members of this cluster are men, lawyers with a normal bmi.\n",
    "\n",
    "In general it can be concluded that the perception of the sleep quality is not directly related with having or not a sleep disorder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_sumary = summarize_clusters(data_pca,'Cluster', numeric_variables, categorical_variables,\"sleep_disorder\")\n",
    "pca_sumary[categorical_variables+['sleep_disorder_percentage']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_sumary[numeric_variables]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kmeans with raw data\n",
    "For this, we need data without strong correlations. The variables related to blood pressure have a high correlation of almost 0.97. To avoid losing any information, we decided to create a new variable called pulse pressure. Pulse pressure is calculated as the difference between systolic and diastolic blood pressure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"pulse_pressure\"] = data['blood_pressure_systolic']-data['blood_pressure_diastolic'] \n",
    "data_k = data.copy()\n",
    "data_kmeans= data[numeric_variables+['pulse_pressure']]\n",
    "data_kmeans = data_kmeans.drop(columns=['blood_pressure_systolic','blood_pressure_diastolic'])\n",
    "data_kmeans[['age', 'sleep_duration', 'physical_activity_level', 'heart_rate',\n",
    "       'daily_steps', 'pulse_pressure']] = StandardScaler().fit_transform(data_kmeans[['age', 'sleep_duration', 'physical_activity_level', 'heart_rate',\n",
    "       'daily_steps', 'pulse_pressure']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "silhouette_scores = []\n",
    "inertia = []\n",
    "for k in range(2, 11):  # Silhouette score is undefined for k=1\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    kmeans.fit(data_kmeans)\n",
    "    score = silhouette_score(data_kmeans, kmeans.labels_)\n",
    "    silhouette_scores.append(score)\n",
    "    inertia.append(kmeans.inertia_)\n",
    "# Plotting the Silhouette Scores\n",
    "fig, axs = plt.subplots(1,2,figsize=(6,2))\n",
    "axs[0].plot(range(2, 11), silhouette_scores)\n",
    "axs[0].set_title('Silhouette Method for Optimal k')\n",
    "axs[0].set_xlabel('Number of Clusters (k)')\n",
    "axs[0].set_ylabel('Silhouette Score')\n",
    "axs[1].plot(range(2, 11), inertia)\n",
    "axs[1].set_title('Elbow Method for Optimal k')\n",
    "axs[1].set_xlabel('Number of Clusters (k)')\n",
    "axs[1].set_ylabel('WCSS')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you compare the two graphs, there is stabilization around 4 clusters in the Silhouette Score graph and a noticeable decrease in the slope (tangent) in the Elbow Method graph. However, using 5 clusters appears slightly better, as it balances both metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4 clusters\n",
    "data_kmeans_5 = data_kmeans.copy()\n",
    "# Step 2: Perform K-Means clustering on the raw data\n",
    "kmeans = KMeans(n_clusters=5, random_state=14, init=\"random\", n_init=30)\n",
    "kmeans.fit(data_kmeans_5)\n",
    "\n",
    "# Add cluster labels to the DataFrame\n",
    "data_kmeans_5['Cluster'] = kmeans.labels_\n",
    "\n",
    "# CHECKING THE CLUSTERS\n",
    "# Get the inertia (WCSS)\n",
    "inertia = kmeans.inertia_\n",
    "print(f\"Inertia (WCSS): {inertia}\")\n",
    "\n",
    "sil_score = silhouette_score(data_kmeans_5, kmeans.labels_)\n",
    "print(f\"Silhouette Score: {sil_score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add cluster labels to the DataFrame\n",
    "data_k['Cluster'] = kmeans.labels_\n",
    "\n",
    "# Create pairplot to visualize the clusters\n",
    "sns.pairplot(data_k[['age', 'sleep_duration', 'physical_activity_level', 'heart_rate',\n",
    "       'daily_steps', 'pulse_pressure','Cluster']], hue='Cluster', palette='Dark2',height=1.5)\n",
    "plt.title('Pairwise Plot of Clusters')\n",
    "plt.show()\n",
    "\n",
    "inertia = kmeans.inertia_\n",
    "print(f\"Inertia (WCSS): {inertia}\")\n",
    "\n",
    "sil_score = silhouette_score(data_kmeans_5, kmeans.labels_)\n",
    "print(f\"Silhouette Score: {sil_score}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a larger decrease in WCSS and a slight improvement in the Silhouette Score when choosing 5 clusters. Additionally, using PCA to visualize the data reveals better separation of the clusters compared to 4 clusters. Choosing 5 clusters reduces overlaps between the groups, providing a more distinct and interpretable clustering structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data description for the cluster is as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_k['sleep_disorder'] = pd.to_numeric(data_k['sleep_disorder'])\n",
    "kmeans_raw_sum = summarize_clusters(data_k,'Cluster', numeric_variables, categorical_variables,'sleep_disorder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_raw_sum[categorical_variables+['sleep_disorder_percentage']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_raw_sum[numeric_variables]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kmeans with mixed data \n",
    "For this, we will also use the pulse pressure variable and exclude the sleep disorder variable. This is because the sleep disorder variable is the target variable that this dataset aims to explain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_kprot = data[numeric_variables+categorical_variables+['pulse_pressure']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the assosiations between numerical and categorical variables finding some interesting relationships:\n",
    "- Age is highly associated with occupation, quality of sleep and stress level.\n",
    "- The hear rate is associated with quality of sleep, stress level and BMI category.\n",
    "- Other important relations are gender with occupation and sleep disorder with bmi category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate only the plot\n",
    "_ = associations(data_kprot, \n",
    "                 nominal_columns=categorical_variables, \n",
    "                 plot=True, \n",
    "                 cmap=\"coolwarm\",\n",
    "                 figsize=(12,6))\n",
    "\n",
    "plt.show()  # Display the plot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_kp = data_kprot.copy()\n",
    "data_kprot[numeric_variables+['pulse_pressure']] = StandardScaler().fit_transform(data_kprot[numeric_variables+['pulse_pressure']])\n",
    "data_kprot = data_kprot.drop(columns=['blood_pressure_systolic','blood_pressure_diastolic'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoders = {}\n",
    "for col in categorical_variables:\n",
    "    le = LabelEncoder()\n",
    "    data_kprot[col] = le.fit_transform(data_kprot[col])\n",
    "    label_encoders[col] = le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_aux = data_kprot.drop(columns=['sleep_disorder'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_array = data_aux.to_numpy()\n",
    "categorical_variables_sub = ['gender',\n",
    " 'occupation',\n",
    " 'quality_of_sleep',\n",
    " 'stress_level',\n",
    " 'bmi_category']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = OneHotEncoder(sparse=False) # sparse_output ---------------------------\n",
    "categorical_encoded = encoder.fit_transform(data_aux[categorical_variables_sub])\n",
    "\n",
    "# Combine the numeric and encoded categorical data\n",
    "numeric_variables_sub = data_aux.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "numeric_data = data_aux[numeric_variables_sub].to_numpy()\n",
    "combined_data = np.hstack((numeric_data, categorical_encoded))\n",
    "\n",
    "cost = []\n",
    "silhouette_scores = []\n",
    "for cluster in range(2, 13):\n",
    "    try:\n",
    "        kprototype = KPrototypes(n_jobs=-1, n_clusters=cluster, n_init=30, init='Huang', random_state=123)\n",
    "        cluster_labels = kprototype.fit_predict(data_aux.to_numpy(), \n",
    "                                                categorical=[data_aux.columns.get_loc(col) for col in categorical_variables_sub])\n",
    "        cost.append(kprototype.cost_)\n",
    "\n",
    "        score_aux = silhouette_score(combined_data, cluster_labels, metric='euclidean')\n",
    "        silhouette_scores.append(score_aux)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error for cluster {cluster}: {e}\")\n",
    "        break\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(6, 2))\n",
    "\n",
    "# Silhouette Score Plot\n",
    "axs[0].plot(range(2, 13), silhouette_scores, marker='o')\n",
    "axs[0].set_title('Silhouette Method for Optimal k')\n",
    "axs[0].set_xlabel('Number of Clusters (k)')\n",
    "axs[0].set_ylabel('Silhouette Score')\n",
    "\n",
    "# Elbow Plot (Cost)\n",
    "axs[1].plot(range(2, 13), cost, marker='o')\n",
    "axs[1].set_title('Elbow Method for Optimal k')\n",
    "axs[1].set_xlabel('Number of Clusters (k)')\n",
    "axs[1].set_ylabel('WCSS')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case the elbow method does not show a clear point where the trend of the line change, but the silhouette method suggest to take 6 or 8 clusters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the categorical variables are properly encoded for K-Prototypes\n",
    "data_kprot8 = data_aux.copy()\n",
    "data_array = data_kprot8.to_numpy()\n",
    "\n",
    "# Perform K-Prototypes clustering\n",
    "kproto = KPrototypes(n_clusters=8, init='Huang', n_init=15, random_state=42)\n",
    "cluster_labels = kproto.fit_predict(data_array, \n",
    "                                    categorical=[data_kprot8.columns.get_loc(col) for col in categorical_variables_sub])\n",
    "\n",
    "# Add cluster labels to the DataFrame\n",
    "data_kprot8['Cluster_8'] = cluster_labels\n",
    "\n",
    "# Get the inertia (WCSS)\n",
    "inertia = kproto.cost_\n",
    "print(f\"Inertia (WCSS): {inertia}\")\n",
    "# Convert categorical variables to numeric\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "categorical_encoded = encoder.fit_transform(data_kprot8[categorical_variables_sub])\n",
    "numeric_data = data_kprot8[['age', 'sleep_duration', 'physical_activity_level', 'heart_rate', 'daily_steps','pulse_pressure']].to_numpy()\n",
    "\n",
    "# Combine numeric and encoded categorical features\n",
    "combined_data = np.hstack((numeric_data, categorical_encoded))\n",
    "\n",
    "# Compute silhouette score\n",
    "sil_score = silhouette_score(combined_data, cluster_labels)\n",
    "print(f\"Silhouette Score: {sil_score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the categorical variables are properly encoded for K-Prototypes\n",
    "data_kprot6 = data_aux.copy()\n",
    "data_array = data_kprot6.to_numpy()\n",
    "\n",
    "# Perform K-Prototypes clustering\n",
    "kproto = KPrototypes(n_clusters=6, init='Huang', n_init=15, random_state=42)\n",
    "cluster_labels = kproto.fit_predict(data_array, \n",
    "                                    categorical=[data_kprot6.columns.get_loc(col) for col in categorical_variables_sub])\n",
    "\n",
    "# Add cluster labels to the DataFrame\n",
    "data_kprot6['Cluster'] = cluster_labels\n",
    "data_kp['Cluster'] = cluster_labels\n",
    "\n",
    "# Create pairplot to visualize the clusters\n",
    "sns.pairplot(data_kprot6[['age', 'sleep_duration', 'physical_activity_level', 'heart_rate', 'daily_steps', 'Cluster', 'pulse_pressure']], \n",
    "             hue='Cluster', palette='Dark2',height=1.5)\n",
    "plt.suptitle('Pairwise Plot of Clusters', y=1.02)\n",
    "plt.show()\n",
    "\n",
    "# Get the inertia (WCSS)\n",
    "inertia = kproto.cost_\n",
    "print(f\"Inertia (WCSS): {inertia}\")\n",
    "# Convert categorical variables to numeric\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "categorical_encoded = encoder.fit_transform(data_kprot6[categorical_variables_sub])\n",
    "numeric_data = data_kprot6[['age', 'sleep_duration', 'physical_activity_level', 'heart_rate', 'daily_steps','pulse_pressure']].to_numpy()\n",
    "\n",
    "# Combine numeric and encoded categorical features\n",
    "combined_data = np.hstack((numeric_data, categorical_encoded))\n",
    "\n",
    "# Compute silhouette score\n",
    "sil_score = silhouette_score(combined_data, cluster_labels)\n",
    "print(f\"Silhouette Score: {sil_score}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we compared the silhouette score and the cost. The best quantity of cluster for the mixed kmeans is 8 clusters. But taking into account the size of the data base, we consider that 8 clusters are too much, so we decide to keep 6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_kp['sleep_disorder'] = pd.to_numeric(data_kp['sleep_disorder'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we see before about the positive association of the bmi category and occupation with having or not sleep disorder is reflected in the clusters 0 and 1 where orverweight and being a Nurse/Salesperson increase the ratio of sleep disorder.\n",
    "\n",
    "From cluster number 4 and the association matrix we can infere that the bmi category is the principal driver for having or not a sleep disorder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_kproto = summarize_clusters(data_kp,'Cluster',numeric_variables,categorical_variables,'sleep_disorder')\n",
    "sum_kproto[categorical_variables+['sleep_disorder_percentage']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_kproto[numeric_variables]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi Dimensional Scaling\n",
    "\n",
    "As alternative to Principal Component Analysis, Multi Dimensional Scaling (MDS) offers a distance based dimension reduction method. Input to MDS are a distance (or dissimilarity) Matrix generated by a chosen distance. This distance has to fulfill the following properties:\n",
    "\n",
    "- Symmetry (of distance between two points, A->B = B->A)\n",
    "- Non-Negativity (distance between two points must be non-negative)\n",
    "- Identity of Indiscernibles (distance A->B is only zero if A=B)\n",
    "- Triangular Inequality (A->C <= A->B->C)\n",
    "\n",
    "## Distance Matrices\n",
    "\n",
    "in the following, we utilize the Mahalanobis distance and the Gower dissimilarity to set up two distance matrices, one of which will then used to apply MDS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_mds = data[numeric_variables+categorical_variables]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distance Matrix: mahalanobis\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import mahalanobis\n",
    "\n",
    "def maha_dist(data,\n",
    "              numeric_variables):\n",
    "    data_numeric = data[numeric_variables]\n",
    "\n",
    "    # covariance matrix + inverse\n",
    "    cov_matrix = np.cov(data_numeric.T)  # Transpose because rows are observations\n",
    "    inv_cov_matrix = np.linalg.inv(cov_matrix)\n",
    "\n",
    "    # empty distance matrix\n",
    "    n = len(data_numeric)\n",
    "    m_distance_mahalanobis = np.zeros((n, n))\n",
    "\n",
    "    # pairwise Mahalanobis distances\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            xi = data_numeric.iloc[i].values\n",
    "            xj = data_numeric.iloc[j].values\n",
    "            m_distance_mahalanobis[i, j] = mahalanobis(xi, xj, inv_cov_matrix)\n",
    "\n",
    "    return m_distance_mahalanobis\n",
    "\n",
    "m_distance_mahalanobis = maha_dist(data_mds,numeric_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distance Matrix: Gowers\n",
    "import gower\n",
    "m_distance_gowers = gower.gower_matrix(data_mds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the corresponding distance matrix gives a subjective idea in the differences in the distance matrices, while it does not deliver any profunder conclusions on a data point level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plot distance Matrices WARNING THIS TAKES A LONG TIME TO RUN\n",
    "fig, axes = plt.subplots(1, 2, figsize=(160, 60))  # Adjust size as needed\n",
    "\n",
    "# Plot the first heatmap\n",
    "sns.heatmap(m_distance_mahalanobis, annot=True, fmt=\".1f\", cmap=\"coolwarm\", cbar=True, ax=axes[0])\n",
    "axes[0].set_title(\"Mahalanobis Distance Matrix\")\n",
    "axes[0].set_xlabel(\"Points\")\n",
    "axes[0].set_ylabel(\"Points\")\n",
    "\n",
    "# Plot the second heatmap\n",
    "sns.heatmap(m_distance_gowers, annot=True, fmt=\".1f\", cmap=\"coolwarm\", cbar=True, ax=axes[1])\n",
    "axes[1].set_title(\"Gower Dissimilarity Matrix\")\n",
    "axes[1].set_xlabel(\"Points\")\n",
    "axes[1].set_ylabel(\"Points\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying Multi Dimension Scaling\n",
    "\n",
    "With the distance/dissimilarity Matrices set up, the MDS is performed using Gowers dissimilarity matrix as it also contains the categorical/binary that can be found in the dataset data, whereas Mahalanobis distance was only created with the numerical variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coorp(D):\n",
    "\n",
    "    n = D.shape[0]\n",
    "    # Centering matrix & double centering distance matrix\n",
    "    H = np.eye(n) - np.ones((n, n)) / n\n",
    "    B = -0.5 * H @ D @ H\n",
    "\n",
    "    # Ensure the matrix is positive semidefinite\n",
    "    eigenvalues = np.linalg.eigvalsh(B)\n",
    "    epsilon = 1e-6\n",
    "    if np.min(eigenvalues) < -epsilon:\n",
    "        raise ValueError(\"Matrix D is not Euclidean and needs preprocessing.\")\n",
    "\n",
    "    # Compute eigenvalues and eigenvectors\n",
    "    eigvals, eigvecs = np.linalg.eigh(B)\n",
    "    eigvals = eigvals[::-1]  # Reverse to descending order\n",
    "    eigvecs = eigvecs[:, ::-1]\n",
    "\n",
    "    # Filter non-zero eigenvalues\n",
    "    positive_eigvals = eigvals > epsilon\n",
    "    vaps = eigvals[positive_eigvals]\n",
    "    principal_components = eigvecs[:, positive_eigvals]\n",
    "\n",
    "    # Compute principal coordinates\n",
    "    Y = principal_components * np.sqrt(vaps)\n",
    "\n",
    "    # Variance explained\n",
    "    percent = (vaps / np.sum(vaps)) * 100\n",
    "    acum = np.cumsum(percent)\n",
    "\n",
    "    return Y, vaps, percent, acum\n",
    "\n",
    "Y, vaps, percent, acum = coorp(m_distance_gowers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having performed the MDS, as a first step the cumulative explained variance is plotted to show how many Principal Coordinates are required to represent the full Variance of the Data Set. The plot below shows, how 80 % of the variance can be covered with 7 Principal Components. Notable is how from the 7th principal coordinate onwards, the added variance decreases significantly as the cumulative variance goes towards 100 % "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Plot cumulative variance explained\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(acum, marker='o', linestyle='-', color='b', linewidth=2)\n",
    "plt.xlabel(\"Eigenvalues\", fontsize=12)\n",
    "plt.ylabel(\"Cumulative Variance (%)\", fontsize=12)\n",
    "plt.title(\"Cumulative Variance Explained\", fontsize=14)\n",
    "plt.axvline(x=7, color='red', linestyle='--', label=\"Vertical line\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the first three principal coordinates, does not give significant insight into how these principal coordinates can be interpreted. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for i in range(0,3):\n",
    "\n",
    "#n  components\n",
    "n_coords = 3\n",
    "fig, axes = plt.subplots(1, n_coords, figsize=(15, 5), sharey=True)\n",
    "\n",
    "axes[0].scatter(Y[:, 0], Y[:, 1], c='b', marker='o', label='Points')\n",
    "axes[0].set_xlabel(f\"Principal Coordinate {1}\", fontsize=12)\n",
    "axes[0].set_ylabel(f\"Principal Coordinate {2}\", fontsize=12)\n",
    "axes[0].set_title(f\"Variance Explained: {acum[1]:.2f}%\", fontsize=14)\n",
    "axes[0].grid()\n",
    "\n",
    "axes[1].scatter(Y[:, 0], Y[:, 2], c='b', marker='o', label='Points')\n",
    "axes[1].set_xlabel(f\"Principal Coordinate {1}\", fontsize=12)\n",
    "axes[1].set_ylabel(f\"Principal Coordinate {3}\", fontsize=12)\n",
    "axes[1].set_title(f\"Variance Explained: {acum[0]+(acum[2]-acum[1]):.2f}%\", fontsize=14)\n",
    "axes[1].grid()\n",
    "\n",
    "axes[2].scatter(Y[:, 1], Y[:, 2], c='b', marker='o', label='Points')\n",
    "axes[2].set_xlabel(f\"Principal Coordinate {2}\", fontsize=12)\n",
    "axes[2].set_ylabel(f\"Principal Coordinate {3}\", fontsize=12)\n",
    "axes[2].set_title(f\"Variance Explained: {acum[2]-acum[0]:.2f}%\", fontsize=14)\n",
    "axes[2].grid()\n",
    "\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To gain a deeper understanding on how to interprete the principal coordinates, the same plot is done with a color coding of the categorical and the binary Variables.\n",
    "\n",
    "- Gender is divided in the third quadrant with the males and higher values in the first and second coordinate with females.\n",
    "- Occupation does not seems to have an specific order in any of the planes.\n",
    "- Quality of sleep is slightly organize over the second coordinate, greater values of that axis represent a better quality of sleep.\n",
    "- Strees level seems to have the opposite organization as quality of sleep (Higher values of the second axis represents lower values of stress level).\n",
    "- BMI and Sleep Disorder are organized over the first axis.\n",
    "\n",
    "This interpretations are an initial approach of how we can interpret the MDS, however we are going to check the influence of the Categorical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_mds_output = data_mds.copy()\n",
    "data_mds_output[[\"MDS1\",\"MDS2\",\"MDS3\"]] = Y[:,0:3]\n",
    "\n",
    "for i, var in enumerate(categorical_variables):\n",
    "    fig, axes = plt.subplots(1, 4, figsize=(15, 5), gridspec_kw={'width_ratios': [1, 1, 1, 0.2]})\n",
    "    \n",
    "    # MDS1 vs MDS2\n",
    "    sns.scatterplot(\n",
    "        ax=axes[0],\n",
    "        data=data_mds_output,\n",
    "        x='MDS1',\n",
    "        y='MDS2',\n",
    "        hue=var,\n",
    "        palette='Set1',\n",
    "        s=100,\n",
    "        edgecolor='black'\n",
    "    )\n",
    "    axes[0].set_title(f'MDS plot colored by {var},\\n Variance Explained: {acum[1]:.2f}%')\n",
    "    axes[0].legend().set_visible(False) \n",
    "\n",
    "    \n",
    "    # MDS1 vs MDS3\n",
    "    sns.scatterplot(\n",
    "        ax=axes[1],\n",
    "        data=data_mds_output,\n",
    "        x='MDS1',\n",
    "        y='MDS3',\n",
    "        hue=var,\n",
    "        palette='Set1',\n",
    "        s=100,\n",
    "        edgecolor='black'\n",
    "    )\n",
    "    axes[1].set_title(f'MDS plot colored by {var},\\n Variance Explained: {acum[0] + (acum[2] - acum[1]):.2f}%')\n",
    "    axes[1].legend().set_visible(False) \n",
    "\n",
    "    # MDS2 vs MDS3\n",
    "    sns.scatterplot(\n",
    "        ax=axes[2],\n",
    "        data=data_mds_output,\n",
    "        x='MDS2',\n",
    "        y='MDS3',\n",
    "        hue=var,\n",
    "        palette='Set1',\n",
    "        s=100,\n",
    "        edgecolor='black'\n",
    "    )\n",
    "    axes[2].set_title(f'MDS plot colored by {var},\\n Variance Explained: {acum[2] - acum[0]:.2f}% ')\n",
    "    axes[2].legend().set_visible(False) \n",
    "\n",
    "    # legend\n",
    "    handles, labels = axes[0].get_legend_handles_labels()\n",
    "    axes[3].legend(handles=handles, labels=labels, loc='center', frameon=False)\n",
    "    axes[3].axis('off') \n",
    "\n",
    "    axes[0].axhline(0)\n",
    "    axes[0].axvline(0)\n",
    "\n",
    "    axes[1].axhline(0)\n",
    "    axes[1].axvline(0)\n",
    "\n",
    "    axes[2].axhline(0)\n",
    "    axes[2].axvline(0)\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correlations between original Variables and first three Principal Coordinates\n",
    "\n",
    "- __First axis:__ Blood pressure, BMI category and sleep disporder have a strong positive correlation.\n",
    "- __Second axis:__ Sleep duration has the largest positive correlation and hear reat the largest negative one.\n",
    "- __Third axis:__ Physical activity is the most important variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr, spearmanr\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "# Cramers V\n",
    "def cramer_v(u, X):\n",
    "    n = len(u)\n",
    "    percentiles = np.percentile(X, [0, 25, 50, 75, 100])\n",
    "    Xcat = np.ones(n)\n",
    "    Xcat[(X > percentiles[1]) & (X <= percentiles[2])] = 2\n",
    "    Xcat[(X > percentiles[2]) & (X <= percentiles[3])] = 3\n",
    "    Xcat[X > percentiles[3]] = 4\n",
    "    \n",
    "    # contingency table\n",
    "    cont_table = pd.crosstab(u, Xcat)\n",
    "    chi2, p, dof, expected = chi2_contingency(cont_table)\n",
    "    \n",
    "    k = len(np.unique(u))\n",
    "    r = len(np.unique(Xcat))\n",
    "    \n",
    "    V = np.sqrt(chi2 / (n * min(k - 1, r - 1)))\n",
    "    return V\n",
    "\n",
    "# Main correlation function\n",
    "def correlaciones2(X, Y, pcuant, pnominal):\n",
    "    collumns = X.columns\n",
    "    \n",
    "    X = X.to_numpy()\n",
    "    p = X.shape[1]\n",
    "    pordinal = p - pcuant - pnominal\n",
    "    corr_table = np.zeros((p, 3))\n",
    "\n",
    "    # quantitative variables\n",
    "    for i in range(pcuant):\n",
    "        corr_table[i, :] = [pearsonr(Y[:, 0], X[:, i])[0], pearsonr(Y[:, 1], X[:, i])[0], pearsonr(Y[:, 2], X[:, i])[0]]\n",
    "\n",
    "    # nominal variables\n",
    "    for i in range(pcuant, pcuant + pnominal):\n",
    "        corr_table[i, :] = [cramer_v(X[:, i], Y[:, 0]), cramer_v(X[:, i], Y[:, 1]), cramer_v(X[:, i], Y[:, 2])]\n",
    "\n",
    "    # ordinal variables\n",
    "    for i in range(pcuant + pnominal, p):\n",
    "        corr_table[i, :] = [spearmanr(Y[:, 0], X[:, i])[0], spearmanr(Y[:, 1], X[:, i])[0], spearmanr(Y[:, 2], X[:, i])[0]]\n",
    "\n",
    "    # Plot\n",
    "    L1 = ['PC1', 'PC2', 'PC3']    \n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    cmap = sns.diverging_palette(250, 15, as_cmap=True)\n",
    "    cax = ax.imshow(corr_table, cmap=cmap, vmin=-1, vmax=1)\n",
    "\n",
    "    ax.set_xticks(np.arange(3))\n",
    "    ax.set_xticklabels(L1)\n",
    "\n",
    "    ax.set_yticks(np.arange(p))  # Set numeric tick positions\n",
    "    ax.set_yticklabels(collumns)  # Use the actual column names as labels\n",
    "\n",
    "    ax.set_title('Principal Coordinates Heatmap', fontsize=12)\n",
    "    fig.colorbar(cax, ax=ax)\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "    return corr_table\n",
    "\n",
    "\n",
    "\n",
    "# excecute\n",
    "corr_table = correlaciones2(X = data_mds[numeric_variables + categorical_variables],#.to_numpy(), \n",
    "               Y = Y[:,0:3], \n",
    "               pcuant = 7, \n",
    "               pnominal = 4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the Influence of Quantitative and Categorical Variables in the MDS configuration computed from Gower's distance we confirm the results obtained with the correlation matrix for the first two axes, and obtain more information:\n",
    "\n",
    "- __Quantitative:__ On the first axis the most influential variables are the blood pressure (distolic and systolic) and the age. While in the second axis sleep duration and heart rate are the most important ones.\n",
    "\n",
    "- __Qualitative:__ The second axis is clearly separated by quality of sleep and stress level. Lowe values of the first axis are influentiated by the gender and higher by the BMI category and sleep disorder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import pdist, squareform\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "\n",
    "\n",
    "def influence(X, p1, p2):\n",
    "    columns = X.columns\n",
    "    X = X.to_numpy()\n",
    "\n",
    "    n, p = X.shape\n",
    "    p3 = p - p1 - p2\n",
    "\n",
    "    # Compute distance matrix\n",
    "    if p2 + p3 == 0:\n",
    "        D = squareform(pdist(X, metric='mahalanobis'))\n",
    "        D2 = D ** 2\n",
    "    else:\n",
    "        D2 = gower.gower_matrix(data)\n",
    "\n",
    "\n",
    "    # Principal coordinates\n",
    "    Y, vaps, _, _ = coorp(D2)\n",
    "    v = 1.0 / vaps[:Y.shape[1]]\n",
    "    Lambda = np.diag(v)\n",
    "    g = np.sum(Y ** 2, axis=1)\n",
    "\n",
    "    m0 = np.min(X, axis=0)\n",
    "    m1 = np.max(X, axis=0)\n",
    "\n",
    "    # evaluate Influence of continious variables \n",
    "    for k in range(p1):\n",
    "        step = (m1[k] - m0[k]) / 50\n",
    "        Xnew = np.arange(m0[k], m1[k] + step, step).reshape(-1, 1)\n",
    "        nXnew = Xnew.shape[0]\n",
    "        virtual = np.zeros((nXnew, p))\n",
    "        virtual[:, k] = Xnew[:, 0]\n",
    "\n",
    "        dnew = np.zeros((nXnew, n))\n",
    "        for i in range(nXnew):\n",
    "            Xvirtual = np.vstack([X, virtual[i, :]])\n",
    "            D2new = gower.gower_matrix(Xvirtual)\n",
    "            dnew[i, :] = D2new[-1, :-1]\n",
    "\n",
    "        ynew = np.zeros((nXnew, Y.shape[1]))\n",
    "        for i in range(nXnew):\n",
    "            ynew[i, :] = 0.5 * (g - dnew[i, :]) @ Y @ Lambda\n",
    "\n",
    "        # plot\n",
    "        plt.figure(1,figsize=(10,5))\n",
    "        plt.plot(ynew[:, 0], ynew[:, 1],linestyle=':', label=f'{columns[k]}', marker='o')\n",
    "        plt.legend(title=\"Continuous Variables\", fontsize=10)\n",
    "        plt.title(\"Influence of Quantitative Variables (Gower)\", fontsize=12)\n",
    "        plt.xlabel('Coordinate 1')\n",
    "        plt.ylabel('Coordinate 2')\n",
    "        plt.grid()\n",
    "\n",
    "    # evaluate Influence of categorical/binary variables \n",
    "    plt.figure(figsize=(10,5))\n",
    "    colors = cm.get_cmap('tab10') \n",
    "    for k in range(p1, p):\n",
    "\n",
    "        # principalcoordinates\n",
    "        Xnew = np.unique(X[:, k]) \n",
    "        nXnew = len(Xnew)\n",
    "        virtual = np.zeros((nXnew, p))\n",
    "        virtual[:, k] = Xnew\n",
    "        dnew = np.zeros((nXnew, n))\n",
    "        \n",
    "        for i in range(nXnew):\n",
    "            Xvirtual = np.vstack([X, virtual[i, :]]) \n",
    "            D2new = gower.gower_matrix(Xvirtual)  \n",
    "            dnew[i, :] = D2new[-1, :-1]\n",
    "\n",
    "        ynew = np.zeros((nXnew, Y.shape[1]))\n",
    "        for i in range(nXnew):\n",
    "            ynew[i, :] = 0.5 * (g - dnew[i, :]) @ Y @ Lambda\n",
    "\n",
    "        # plot\n",
    "        plt.plot(\n",
    "            ynew[:, 0], ynew[:, 1], linestyle=':', marker='o', markersize=10, \n",
    "            label=f'{columns[k]}', color=colors(k-6)\n",
    "        )\n",
    "        plt.title(f'Influence of Categorical and Binary Variables (Gower)')\n",
    "        plt.legend(title=\"Categorical and Binary Variables\", fontsize=10)\n",
    "        plt.title(\"Influence of Quantitative Variables (Gower)\", fontsize=12)\n",
    "        plt.xlabel('Coordinate 1')\n",
    "        plt.ylabel('Coordinate 2')\n",
    "        plt.grid()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# encode categorical/binaryvariables \n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "data_encoded = data_mds.copy()\n",
    "for col in categorical_variables:\n",
    "    data_encoded[col] = label_encoder.fit_transform(data[col])\n",
    "\n",
    "\n",
    "# run\n",
    "influence(X=data_encoded[numeric_variables+categorical_variables], p1= 7, p2 = 4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Looking at the findings of the Multi Dimensiona Scaling with Gowers distance, it is apparent that the MDS compared to the PCA, is not able to contain as much of the information (variance) in a small number of variables. Where the PCA was able to represent 90 % of the variability with 3 principal components, the MDS requires 6 principal coordinates to represent 80% of the variability with significant more to reach the respective 90%. With a total of 13 Variables, when the objective is the maximum possible dimension reduction, MDS thus appear to be the significantly better approach. None the less, the interpretability of the resulting new dimensions might however favour MDS, where the Influence of the categorical variables shows to be nearly orthogonal for the PC1 vs PC2 dimensions. \n",
    "\n",
    "The MDS could be further improved by modifying its configuration with new distance types as done in ReIMS, using different distances from the traditional Gower's distance to archieve greater robustness & stability. This however would not change the fact, that PCA appears to be better fit for dimension reduction in this particular data set. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
