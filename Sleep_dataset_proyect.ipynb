{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "tags": [
     "hide_code",
     "h"
    ]
   },
   "outputs": [],
   "source": [
    "### import libraries\n",
    "\n",
    "# format\n",
    "import pandas as pd\n",
    "\n",
    "# plot\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "\n",
    "# math\n",
    "import numpy as np\n",
    "import math\n",
    "from scipy.linalg import inv, det\n",
    "from numpy.linalg import eig\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "#PCA\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "#### Authors: Laura Silvana Alvarez, Florencia Maite Luque and Simon Schmetz\n",
    "The following project documentation was written as work assignment for the module \"Multivariate Analysis\" of the Master in Statistics for Data Science at the Universidad Carlos III de Madrid. It contains the Multivariate Analysis of a Kaggel dataset on Sleep Health and Lifestyle (https://www.kaggle.com/datasets/uom190346a/sleep-health-and-lifestyle-dataset). The work is split into two parts, where in a first part a exploratory data analysis is performed, some data preprocessing steps are taken and a Prinicipal Component Analysis (PCA) is performed. In the second part, distance based metrics will then be applied to identify clusters emerging from the PCA. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "tags": [
     "hide_code"
    ]
   },
   "outputs": [],
   "source": [
    "# load data\n",
    "data_raw = pd.read_csv(\"Sleep_health_and_lifestyle_dataset.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The dataset at hand is composed out of the collumns shown in the table below. It has been modified compared to the kaggle source data by turning the \"Sleep Disorder\" Variable into a binary variable (yes/no) and by seperating the blood pressure variables into the two variables blood pressure systolic and blood pressure diastolic. The resulting data set is composed out of the variables shown in the following table. \n",
    "\n",
    "| **Variable**                    | **Description**                                                                                              |\n",
    "|---------------------------------|--------------------------------------------------------------------------------------------------------------|\n",
    "| Person ID                       | An identifier for each individual.                                                                           |\n",
    "| Gender                          | The gender of the person (Male/Female).                                                                      |\n",
    "| Age                             | The age of the person in years.                                                                              |\n",
    "| Occupation                      | The occupation or profession of the person.                                                                  |\n",
    "| Sleep Duration (hours)          | The number of hours the person sleeps per day.                                                               |\n",
    "| Quality of Sleep (scale: 1-10)  | A subjective rating of the quality of sleep, ranging from 1 to 10.                                          |\n",
    "| Physical Activity Level (minutes/day) | The number of minutes the person engages in physical activity daily.                               |\n",
    "| Stress Level (scale: 1-10)      | A subjective rating of the stress level experienced by the person, ranging from 1 to 10.                    |\n",
    "| BMI Category                    | The BMI category of the person (e.g., Underweight, Normal, Overweight).                                      |\n",
    "| Blood Pressure (systolic) | The blood pressure measurement of the person (systolic pressure)|\n",
    "| Blood Pressure (diastolic) |  The blood pressure measurement of the person (diastolic pressure)|\n",
    "| Heart Rate (bpm)                | The resting heart rate of the person in beats per minute.                                                    |\n",
    "| Daily Steps                     | The number of steps the person takes per day.                                                                |\n",
    "| Sleep Disorder                  | The presence or absence of a sleep disorder in the person (Binary)|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "tags": [
     "hide_code"
    ]
   },
   "outputs": [],
   "source": [
    "### Preprocess Data\n",
    "data = data_raw.copy()\n",
    "\n",
    "# rename columns for easier use\n",
    "rename_dict = {\n",
    "    'Person ID':'person_id',\n",
    "    'Gender': 'gender',\n",
    "    'Age':'age',\n",
    "    'Occupation':'occupation',\n",
    "    'Sleep Duration':'sleep_duration',\n",
    "    'Quality of Sleep':'quality_of_sleep',\n",
    "    'Physical Activity Level':'physical_activity_level',\n",
    "    'Stress Level':'stress_level',\n",
    "    'BMI Category':'bmi_category', \n",
    "    'Blood Pressure':'blood_pressure', \n",
    "    'Heart Rate':'heart_rate', \n",
    "    'Daily Steps':'daily_steps',\n",
    "    'Sleep Disorder':'sleep_disorder' \n",
    "}\n",
    "data.rename(columns=rename_dict, inplace=True)\n",
    "\n",
    "# change dtype\n",
    "data['quality_of_sleep'] = data['quality_of_sleep'].astype(str)\n",
    "data['stress_level'] = data['stress_level'].astype(str)\n",
    "\n",
    "# make sleep disorder binary\n",
    "data['sleep_disorder'] = data['sleep_disorder'].map(lambda x: '1' if x in ['Insomnia','Sleep Apnea'] else '0').astype(str)\n",
    "\n",
    "# split blood pressure into diastolic & systolic\n",
    "data[[\"blood_pressure_systolic\",\"blood_pressure_diastolic\"]] = data[\"blood_pressure\"].str.split('/',expand=True)\n",
    "data[\"blood_pressure_diastolic\"] = pd.to_numeric(data['blood_pressure_diastolic'])\n",
    "data[\"blood_pressure_systolic\"] = pd.to_numeric(data[\"blood_pressure_systolic\"])\n",
    "\n",
    "\n",
    "#data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on unique value counts and the variable description, the numeric and categorical variables are identified as: \n",
    "\n",
    "| **Type**              | **Variables**                                                       |\n",
    "|------------------------|---------------------------------------------------------------------|\n",
    "| **Numeric Variables**  | age, sleep_duration, physical_activity_level, heart_rate, daily_steps, blood_pressure_systolic, blood_pressure_diastolic |\n",
    "| **Categorical Variables** | gender, occupation, quality_of_sleep, stress_level, bmi_category, sleep_disorder |\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # set up column lists\n",
    "numeric_variables = ['age','sleep_duration','physical_activity_level','heart_rate','daily_steps','blood_pressure_systolic','blood_pressure_diastolic']\n",
    "categorical_variables = ['gender','occupation','quality_of_sleep','stress_level','bmi_category','sleep_disorder']\n",
    "# data.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data[categorical_variables].apply(pd.unique)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reviewing the unique values of the identified categorical values, a duplicate in bmi_category in the values \"Normal\" and \"Normal Weight\" can be identified. This is solved by replacing all instances of \"Normal Weight\" with \"Normal\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['bmi_category'] = data['bmi_category'].replace('Normal Weight', 'Normal')\n",
    "#data[[\"bmi_category\"]].apply(pd.unique)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doing so concludes the required initial preprocessing steps and allows to begin with the first part of this project work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1 - Exploratory Analysis and Dimension Reduction via PCA\n",
    "The first part of this work contains the initial exploratory analysis of the dataset as well as a Principal Component Analysis (PCA) of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Exploratory Data Analysis contains a general overview of the datasets structure and correlations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show first rows of dataframe\n",
    "#data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The categorical Variables can be further sperated into the following categories with: \n",
    "\n",
    "Two Binary variable:\n",
    "- gender\n",
    "- sleep disorder\n",
    "\n",
    "Three ordinal variables: \n",
    "- quality of sleep\n",
    "- stress level\n",
    "- bmi_category\n",
    "\n",
    "And one nominal variables:\n",
    "- occupation\n",
    "\n",
    "Plotting barplots for the categorical variables, beginning with the binary variables of gender and sleep disorder, shows an even distribution between male and female and a fairly even distribution between observations with and without sleep disorders. In the meantime, stress level shows a decrease in observations towards higher stress levels, and equally, quality of sleep and body mass index (BMI) show a continuous decrease in observations for quality of sleep from 8 to 4 and for BMI categories from normal to obese. Lastly, the variable occupation shows a somewhat uneven distribution of observations between the different occupations, with the two occupations that individually contribute the most to the overall dataset being Nurses and Doctors. The bias of potential overrepresentation of the medical field with irregular working hours in shifts cannot be further analyzed in this work but has to be taken into account in later conclusions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Barplots for categorical\n",
    "def pie_bar(data, features):\n",
    "    n_features = len(features)\n",
    "    n_cols = 3 \n",
    "    n_rows = (n_features + n_cols - 1) // n_cols\n",
    "    \n",
    "    fig, axs = plt.subplots(n_rows, n_cols, figsize=(10, 3 * n_rows))\n",
    "    axs = axs.flatten()\n",
    "    \n",
    "    for idx, feature in enumerate(features):\n",
    "        count_values = data[feature].value_counts()\n",
    "        axs[idx].bar(x=count_values.index, height=count_values.values, edgecolor='black')\n",
    "        axs[idx].set_xticks(range(len(count_values)))\n",
    "        axs[idx].set_xticklabels(count_values.index, rotation=45, ha='right')\n",
    "        axs[idx].set_title(f\"Bar Plot for {feature}\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "pie_bar(data, categorical_variables)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of the seven numerical variables, all are continuous. Plotting histograms and boxplots side by side, two categories emerge:\n",
    "\n",
    "- Measured variables\n",
    "- Estimated/rounded variables (variables that were probably estimated as part of a questionnaire by participants)\n",
    "\n",
    "For the measured variables, age, heart rate, and sleep duration, a more or less continuous distribution can be identified. Age, while showing variance, still is somewhat uniformly distributed between the limits of around 30 to 60. Sleep duration appears to show three groups: one group having very little sleep (up to 6.5 hours) and one group sleeping for longer times (more than 8 hours), with a dip in sleeping time observed between these two groups. As the last variable of this group, the heart rate is shown to have the majority of its observations between 65 and 75, with visual hints of a normal distribution. However, it shows significant outliers for higher heart rates in both the histogram and boxplot, which will need to be addressed in the preprocessing step.\n",
    "The estimated/rounded variables (physical activity level, daily steps, blood pressure systolic/diastolic) show a specific characteristic with oscillations between highs and lows, attributed to the way people estimate numeric values in increments, like evaluating physical activity level (mins/day) mostly in increments of 15 minutes (half an hour, 45 minutes, one hour, etc.). The lows in between then show observations with more specific answers like 42 minutes, leading to the somewhat irregular appearance of the histograms. Taking this into account, the physical activity level shows a fairly uniform distribution of observations, while daily steps tend more toward a right-skewed uniform distribution, indicating an overall potentially above-average fit sample of people. The high percentage of medical workers with great walking distances as part of their profession might further contribute to this. Meanwhile, the blood pressure systolic/diastolic appears to be more or less evenly distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Histograms + Boxplot\n",
    "def hist_box_plot(data, numeric_variables):\n",
    "    fig, axs = plt.subplots(2, len(numeric_variables), figsize=(len(numeric_variables)*5, 5))\n",
    "    for i, feature in enumerate(numeric_variables):\n",
    "        \n",
    "        # Plot histogram\n",
    "        axs[0, i].hist(data[feature], bins=10, density=True, alpha=0.6, color='b', edgecolor=\"black\")\n",
    "        axs[0, i].set_title(f\"Hist of {feature}\")\n",
    "\n",
    "        # Plot boxplot\n",
    "        axs[1, i].boxplot(data[feature], vert=False, patch_artist=True, medianprops=dict(color=\"black\"))\n",
    "        axs[1, i].set_title(f\"Boxplot of {feature}\")\n",
    "    \n",
    "    plt.tight_layout(pad=2.0)\n",
    "\n",
    "hist_box_plot(data, numeric_variables[0:3])\n",
    "hist_box_plot(data, numeric_variables[3:5])\n",
    "hist_box_plot(data, numeric_variables[5:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a first attempt to evaluate the correlations found in this dataset, the following set of Metrics is applied and plotted.\n",
    "\n",
    "\n",
    "\\begin{aligned}\n",
    "q_1 &= \\left(1 - \\frac{\\min \\lambda_j}{\\max \\lambda_j} \\right)^{p+2}, & \\quad q_4 &= \\left(\\frac{\\max \\lambda_j}{p} \\right)^{3/2}, \\\\\n",
    "q_2 &= 1 - \\frac{p}{\\sum_{j=1}^p \\left(1/\\lambda_j\\right)}, & \\quad q_5 &= \\left(1 - \\frac{\\min \\lambda_j}{p} \\right)^5, \\\\\n",
    "q_3 &= 1 - \\sqrt{|R|}, & \\quad q_6 &= \\sum_{j=1}^p \\frac{1 - 1/r_{ij}}{p}.\n",
    "\\end{aligned}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Statistical Analysis & intercorrelation\n",
    "\n",
    "means = data[numeric_variables].mean()\n",
    "variances = data[numeric_variables].var()\n",
    "df_covariance_matrix = data[numeric_variables].cov()\n",
    "df_correlation_matrix = data[numeric_variables].corr()\n",
    "\n",
    "def intercorrelations(X,categorical_filter = \"whole Dataset\",label = None):\n",
    "    n, p = X.shape\n",
    "    R = np.corrcoef(X, rowvar=False)\n",
    "    lambda_vals, _ = eig(R)\n",
    "    rjj = np.diag(inv(R))\n",
    "    q = np.zeros(6)\n",
    "    q[0] = (1 - min(lambda_vals) / max(lambda_vals)) ** (p + 2)\n",
    "    q[1] = 1 - p / np.sum(1. / lambda_vals)\n",
    "    q[2] = 1 - np.sqrt(det(R))\n",
    "    q[3] = (max(lambda_vals) / p) ** (3 / 2)\n",
    "    q[4] = (1 - min(lambda_vals) / p) ** 5\n",
    "    q[5] = np.sum((1 - 1. / rjj) / p)\n",
    "    \n",
    "    # plot\n",
    "    plt.plot(range(1, 7), q, marker='o', linestyle='-', label=label)\n",
    "    plt.xlabel(\"intercorrelation metric\")\n",
    "    plt.ylabel(\"intercorrelation score\")\n",
    "    plt.suptitle(\"Intercorrelation Metric for filter: \"+categorical_filter)\n",
    "    plt.legend()\n",
    "\n",
    "    return None\n",
    "\n",
    "plt.figure(figsize=(5,2.5))\n",
    "intercorrelations(data[numeric_variables])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying the same method to a dataset filtered on the binary variable sleep disorder shows an overall higher than before correlation in the subset with sleep disorder while the subset for people without sleep disorder shows lower correlation metrics than the joined dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#intercorrelation by category function\n",
    "def inter_by_category(df,cat):\n",
    "    count_values = df[cat].value_counts().index\n",
    "    plt.figure(figsize=(5,2.5))\n",
    "    for i in count_values:\n",
    "        aux_data = df[df[cat]==i]\n",
    "        intercorrelations(aux_data[numeric_variables],categorical_filter=cat,label=i)\n",
    "\n",
    "inter_by_category(data,'sleep_disorder')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expanding the correlation analysis with a pair plot shows a variety of potential correlations between variables, the most notable being the linear correlation between blood pressure systolic and diastolic, as well as between daily steps and heart rate/blood pressure. Further correlation can be seen between physical activity level and sleep duration, while the plot of age vs. sleep duration appears to show certain clusters that may be further analyzed in the second part of this work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pairplot\n",
    "sns.set(rc={'figure.figsize':(6, 3)})\n",
    "sns.pairplot(data[numeric_variables], diag_kind='kde', corner=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the correlation corresponding matrix reflects some of the observations made in the pairplot, with the verious near 100% correlation between blood pressure systolic and diastolic very aparent. Two previously less apparent correlations are the ones between age and blood pressure as well as the correlation between physical activity and daily steps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_mat = data[numeric_variables].corr()\n",
    "plt.figure(figsize=(4, 3))  # Adjust the figure size as needed\n",
    "sns.heatmap(corr_mat, annot=True, cmap='coolwarm', vmin=-1, vmax=1, center=0, square=True, linewidths=0.5)\n",
    "plt.title('Correlation Plot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the covariance matrix however shows an issue with the current format of the data, where daily steps outweighs all other variances due to its scale (3000 - 10000). This issue will be adressed in the next section of part one of this work. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cov_plot(data,numeric_variables):\n",
    "    corr_mat = data[numeric_variables].cov()\n",
    "    plt.figure(figsize=(4, 3))  # Adjust the figure size as needed\n",
    "    sns.heatmap(corr_mat, annot=True, cmap='coolwarm', square=True, linewidths=0.5)\n",
    "    plt.title('Covariance Matrix')\n",
    "    plt.show()\n",
    "\n",
    "cov_plot(data,numeric_variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having a general overview of structure and correlation in the data, the next step is to scaling and outlier issues in the next subsection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Function for plotting of conditional histograms \n",
    "# def plot_categorical_hist(data,\n",
    "#                           ncols,\n",
    "#                           numeric_variables,\n",
    "#                           categorical_variables,\n",
    "#                           host_stat='count',\n",
    "#                           figsize=(12, 10)\n",
    "#                           ):\n",
    "    \n",
    "#     nrows = math.ceil(len(numeric_variables)/ncols)\n",
    "#     fig, axes = plt.subplots(nrows, ncols, figsize=figsize)\n",
    "#     for i, col in enumerate(numeric_variables):\n",
    "#         row = i // 2 \n",
    "#         col_pos = i % 2 \n",
    "#         sns.histplot(data=data, x=col, bins=10,hue=categorical_variables, kde=True, ax=axes[row, col_pos],stat=host_stat)\n",
    "#         axes[row, col_pos].set_title(f'Distribution of {col}')\n",
    "\n",
    "#     # Adjust layout for better spacing\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n",
    "\n",
    "# # TODO: Select relevant ones \n",
    "# for categorical_variable in categorical_variables:\n",
    "#     print(categorical_variable)\n",
    "#     plot_categorical_hist(data=data,\n",
    "#                           ncols=2,\n",
    "#                           numeric_variables=numeric_variables,\n",
    "#                           categorical_variables=categorical_variable,\n",
    "#                           host_stat='probability',\n",
    "#                           figsize=(12, 20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "\n",
    "The following two issues in the current data set: \n",
    "- Outliers in variable \"heart rate\"\n",
    "- Scaling issue to (among others) variable \"daily steps\"\n",
    "\n",
    "This section corrects outliers, validates skewness and standardizes the numeric variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outliers and Skewness\n",
    "\n",
    "The aim of this part of the preprocessing, is to obtain symmetric variables without outliers in order to apply in a correct form the PCA. \n",
    "\n",
    "It is observed that only one variable has outliers and positive skewness problems (heart rate). Therefore, the first step is to cut the outliers (4% of the dataframe), and then, check if the skewness problem is also corrected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print skewness\n",
    "for i in numeric_variables:\n",
    "    aux_skew = stats.skew(data[i])\n",
    "    print(f\"Skewness of {i} : {aux_skew}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_stats = []\n",
    "\n",
    "for i in numeric_variables:\n",
    "    # Compute the descriptive statistics using scipy.stats.describe\n",
    "    description = stats.describe(data[i])\n",
    "    p90 = np.percentile(data[i], 90)\n",
    "    p95 = np.percentile(data[i], 95)\n",
    "    p99 = np.percentile(data[i], 99)\n",
    "\n",
    "    # Store the results as a dictionary (with variable name as key)\n",
    "    summary_stats.append({\n",
    "        'Variable': i,\n",
    "        'Count': description.nobs,\n",
    "        'Min': description.minmax[0],\n",
    "        'Mean': description.mean,\n",
    "        'Percentile 90%': p90,\n",
    "        'Percentile 95%': p95,\n",
    "        'Percentile 99%': p99,\n",
    "        'Max': description.minmax[1],\n",
    "        'Variance': description.variance\n",
    "    })\n",
    "\n",
    "# Convert the list of dictionaries to a DataFrame for easier display\n",
    "summary_df = pd.DataFrame(summary_stats)\n",
    "\n",
    "# Display the summary statistics table\n",
    "#print(summary_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in numeric_variables:\n",
    "    q1 = np.percentile(data[i], 25)\n",
    "    q3 = np.percentile(data[i], 75)\n",
    "    RIC = q3 - q1 \n",
    "    nout = np.sum(data[i] > (q3 + 1.5*RIC))\n",
    "    print(f\"The threshold for {i} upper outliers is  {q3 + 1.5*RIC}\")\n",
    "    print(f\" then there are {nout} outliers in this variable, representing the {np.round(nout/374*100,2)} % of the dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the skewness was also corrected by cutting the oultiers observations. For that reason, there is not needed another type of transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temporary data while we have all the transformations\n",
    "data_cut = data[data['heart_rate'] < 78] \n",
    "\n",
    "# CHECKING SKEWNESS AFTER CUTTING OUTLIERS\n",
    "aux_skew = stats.skew(data_cut[\"heart_rate\"])\n",
    "print(f\"Skewness of heart_rate : {aux_skew}\")\n",
    "\n",
    "\n",
    "# TODO: Add x axis label, turn into two big plots\n",
    "\n",
    "# function for histogram + boxplots on numerical variables\n",
    "def hist_box_plot_basic(data, feature):\n",
    "    fig, axs = plt.subplots(1,2, figsize=(5, 2.5))\n",
    "    axs[0].hist(data[feature],bins=10,density=True,alpha=0.6,color='b',edgecolor=\"black\")\n",
    "    axs[0].set_title(f\"Hist of {feature}\")\n",
    "    axs[1].boxplot(data[feature],vert=False, patch_artist=True,medianprops=dict(color=\"black\"))\n",
    "    axs[1].set_title(f\"Boxplot of {feature}\")\n",
    "    plt.tight_layout(pad=2.0)\n",
    "\n",
    "\n",
    "# plot hist % boxplot before and after\n",
    "hist_box_plot_basic(data,\"heart_rate\")\n",
    "hist_box_plot_basic(data_cut,\"heart_rate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardize numeric variables\n",
    "\n",
    "Having seen in the exploratory data analysis that there exists a strong imbalance in scale between numerical variables in the dataset, the dataset is standardized in this step to mean 0 and scaled on its standarddeviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale\n",
    "data_standardized_numeric = (data_cut[numeric_variables] - data_cut[numeric_variables].mean()) / data_cut[numeric_variables].std()\n",
    "data_standardized = pd.concat([data_standardized_numeric,data_cut[categorical_variables]], axis =1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing the boxplots pre-standardized and post-standardized shows the major impact the rescaling has, where daily steps previously dominated and now an even distribution for all numeric variables can be seen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot\n",
    "fig, axs = plt.subplots(1,2, figsize=(5, 2.5))\n",
    "axs[0].boxplot(data[numeric_variables],vert=True, patch_artist=True,medianprops=dict(color=\"black\"))\n",
    "axs[0].set_xticklabels(numeric_variables, rotation=45, ha=\"right\")\n",
    "axs[1].boxplot(data_standardized_numeric[numeric_variables],vert=True, patch_artist=True,medianprops=dict(color=\"black\"))\n",
    "axs[1].set_xticklabels(numeric_variables, rotation=45, ha=\"right\")\n",
    "fig.suptitle(\"Boxplots not-standardized vs standardized\", fontsize=16)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a result from scaling, now the covariance matrix can be constructed, showing similar results compared to the previously analyzed correlation matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cov_plot(data_standardized,numeric_variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA\n",
    "\n",
    "Having analyzed the data and its characteristic and highly correlated variables identified, as well as having eliminated outliers as well as having standardized the numeric variables, principal component analysis can now be applied in an attempt to reduce dimensionality. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of Principal Components\n",
    "\n",
    "By analysing the Explained Variance (eigenvalues) trend, and the Joliffe´s and Kaiser´s criterion, for this project there are selected 3 Principal components that explain the 90\\% of the variability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pca = data_standardized[numeric_variables]\n",
    "#data_pca.to_csv('C:/Users/Usuario/Desktop/UC3M/SC2/MULTIVARIADO/PROYECTO/data_estandarizada.csv', index=False)\n",
    "pca = PCA()\n",
    "pca.fit(data_pca)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eig_val = pca.explained_variance_\n",
    "#print(\"Explained Variance: \", eig_val)\n",
    "\n",
    "sv = pca.singular_values_\n",
    "#print(\"Singular Values: \", sv)\n",
    "\n",
    "#----------------------- JUST CHECKING MATHS -----------------------\n",
    "singular_values_squared = sv ** 2\n",
    "# Compare singular values squared vs eigenvalues\n",
    "#normalized_singular_values_squared = singular_values_squared / (X_scaled.shape[0] - 1)\n",
    "#print(\"Eigenvalues (Explained Variance):\", eig_val)\n",
    "#print(\"Singular Values Squared (Normalized):\", normalized_singular_values_squared)\n",
    "#--------------------------------------------------------------------------------\n",
    "\n",
    "# Covariance Matrix\n",
    "S = np.cov(data_pca, rowvar=False)\n",
    "print(\"trace: \", S.trace())\n",
    "\n",
    "# Kaiser´s criterion\n",
    "mean_eig = S.trace()/7\n",
    "\n",
    "# Jollife´s criterion\n",
    "joliffe = 0.7 * S.trace()/7\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(5, 2.5))\n",
    "plt.plot(eig_val, marker = 'o', linestyle = '-', color = 'g')\n",
    "plt.axhline(y=mean_eig, color='r', linestyle='--', label='Keisers Criterion')\n",
    "plt.axhline(y=joliffe, color='r', linestyle='-', label='Jollife Criterion')\n",
    "plt.title(\"Explained Variance\")\n",
    "plt.xlabel(\"Principal Component\")\n",
    "plt.ylabel(\"Eigen Values\")\n",
    "\n",
    "# Change xlabels\n",
    "tick_positions = [0, 1, 2, 3, 4, 5, 6]\n",
    "custom_labels = ['PC1', 'PC2', 'PC3', 'PC4', 'PC5', 'PC6', 'PC7']  \n",
    "\n",
    "# Set custom x-ticks with custom labels\n",
    "plt.xticks(ticks=tick_positions, labels=custom_labels)\n",
    "\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# I AM STILL CHECKING THIS ONE, ONLY TRUST 100% THE ONE THAT IS ABOVE\n",
    "\n",
    "\n",
    "# Explained variance ratio\n",
    "explained_variance_ratio = pca.explained_variance_ratio_\n",
    "cumulative_explained_variance = np.cumsum(explained_variance_ratio)\n",
    "\n",
    "# Show the cumulative explained variance\n",
    "plt.figure(figsize=(5, 2.5))\n",
    "plt.plot(range(1, len(cumulative_explained_variance) + 1), cumulative_explained_variance, marker='o', color='g')\n",
    "plt.axvline(x=3, color='r', linestyle='--', label='')\n",
    "plt.title(\"Cumulative Explained Variance\")\n",
    "plt.xlabel(\"Principal Component Index\")\n",
    "plt.ylabel(\"Cumulative Explained Variance Ratio\")\n",
    "\n",
    "# Adding labels next to the points\n",
    "for i, (x, y) in enumerate(zip(range(1, len(cumulative_explained_variance) + 1), cumulative_explained_variance)):\n",
    "    plt.text(x, y, f'{y:.2f}', horizontalalignment='right', verticalalignment='bottom', fontsize=10)\n",
    "\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calc delta variance\n",
    "delta_explained_variance  = []\n",
    "i = 0\n",
    "while i < len(cumulative_explained_variance)-1:\n",
    "    delta_explained_variance.append(cumulative_explained_variance[i+1]-cumulative_explained_variance[i])\n",
    "    i = i+1\n",
    "delta_explained_variance.insert(0, cumulative_explained_variance[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|    **Value**      |  **PC1**   |  **PC2**   |  **PC3**  |  **PC4**   |  **PC5**   |  **PC6**   |  **PC7**  |\n",
    "|-------------------|------------|------------|-----------|------------|------------|------------|-----------|\n",
    "| **Eigen Values**  | 2.98968 | 1.91745 | 1.40832 | 0.35319 | 0.21651 | 0.09865 | 0.01619 |\n",
    "| **% Variability** | 42.7097 | 27.3922 | 20.1188 | 5.04559 | 3.09299 | 1.40933 | 0.23132 |\n",
    "| **Cum. Variance** | 42.7097 | 70.1019 | 90.2207 | 95.2663 | 98.3593 | 99.7686 | 100.000 |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate transformed Data Set\n",
    "pca_transformed = pca.transform(data_pca)\n",
    "df_pca_transformed =pd.DataFrame(pca_transformed,columns=[\"PC1\",\"PC2\",\"PC3\",\"PC4\",\"PC5\",\"PC6\",\"PC7\"])\n",
    "df_pca_merged = pd.concat([df_pca_transformed, data_cut[categorical_variables].reset_index()],axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contribution of Variables to Principal Components and Interpretation\n",
    "\n",
    "Plotting the Composition of the Principal Components as a Barplot shows a general overlap in the contribution of all variables to the new found principle components, making a very clear seperation into characteristics that are described by the prinicipal components a more dificult task. Since all outliers and scale differences have been treated in the preprocessing steps, the dataset simply seems not to have directly obvious meaningfull interpretation of its principal components. While interpreability in more general terms is one of the challenges associated to PCA, an attempt is made to give some intuition to the three principal component dimensions selected to represent this data set. The first principal component PC1 can be interpreted as overall characteristics of a person, with all variables except for sleep_duration somewhat evenly related. PC2 could be interpreted as the dimension of age, with age, leep duration and heart rate mainly contributing. Simelarly, PC3 could be interpreted as the physical condition dimension, with physical activity level, daily steps and sleep duration primarily contributing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eigsort(S):\n",
    "    # Eigenvalues and eigenvectors\n",
    "    eigenvalues, eigenvectors = eig(S)\n",
    "    # Sort Eigenvalues\n",
    "    sorted_indices = np.argsort(eigenvalues)[::-1]\n",
    "    sorted_eigenvalues = eigenvalues[sorted_indices]\n",
    "    sorted_eigenvectors = eigenvectors[:, sorted_indices]\n",
    "    Lambda = sorted_eigenvalues\n",
    "    \n",
    "    return sorted_eigenvectors, Lambda\n",
    "\n",
    "def contribution_plot(ax, arrays, colors, labels, x_labels, width=0.2):\n",
    "    n_arrays = len(arrays)\n",
    "    x_indices = np.arange(len(arrays[0])) + 1\n",
    "\n",
    "    # Barplots\n",
    "    for i, (array, color, label) in enumerate(zip(arrays, colors, labels)):\n",
    "        bar_positions = x_indices + (i - n_arrays / 2) * width\n",
    "        ax.bar(bar_positions, array, width=width, color=color, label=label)\n",
    "\n",
    "    # Cosmetics\n",
    "    ax.set_xlabel(\"$X_n$ PCA Weights (computed from S)\")\n",
    "    ax.set_ylabel(\"Weight\")\n",
    "    ax.set_title(\"PC Composition\")\n",
    "    ax.axhline(0, color='black', linewidth=0.8, linestyle='-') \n",
    "    ax.set_xticks(x_indices)\n",
    "    ax.set_xticklabels(x_labels, rotation=30, ha=\"right\")\n",
    "    ax.legend()\n",
    "\n",
    "[T, Lambda] = eigsort(S)\n",
    "\n",
    "# Correlation Matrix\n",
    "correlation_matrix = pd.DataFrame(\n",
    "    {col2: [data_pca[col1].corr(df_pca_merged[col2]) for col1 in numeric_variables] for col2 in [\"PC1\",\"PC2\",\"PC3\"]},\n",
    "    index=data_pca.columns\n",
    ")\n",
    "\n",
    "# Create Figure\n",
    "fig, axs = plt.subplots(1, 2, figsize=(16, 6),gridspec_kw={'width_ratios': [2.5, 1]})\n",
    "\n",
    "# Brplot (Contribution)\n",
    "contribution_plot(\n",
    "    ax=axs[0],\n",
    "    arrays=[T[:, 0], T[:, 1], T[:, 2]],\n",
    "    colors=[\"royalblue\", \"indianred\", \"seagreen\"],\n",
    "    labels=[\"PC1 Weight\", \"PC2 Weight\", \"PC3 Weight\"],\n",
    "    x_labels=numeric_variables,\n",
    "    width=0.25\n",
    ")\n",
    "\n",
    "# Correlation Plot\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap=\"coolwarm\", fmt=\".2f\", cbar=True, ax=axs[1])\n",
    "axs[1].set_title(\"Correlation Matrix: Numeric Columns vs PC\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the data transformed into the three first principal components shows a similarly entangled image image as already shown in the barplots, with no obious interpretation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def scatterplot_pc(df, x_col, y_col, hue_col=None, ax=None, expl_var=None):\n",
    "    # plot\n",
    "    sns.scatterplot(data=df, \n",
    "                    x=x_col, \n",
    "                    y=y_col, \n",
    "                    hue=hue_col,\n",
    "                    palette='Set1',\n",
    "                    ax=ax)\n",
    "    \n",
    "    # cosmetics\n",
    "    ax.set_title(f'Scatter Plot of {y_col} vs {x_col} by \\n filter: {hue_col}, joined explaind variance {expl_var} %')\n",
    "    ax.set_xlabel(x_col)\n",
    "    ax.set_ylabel(y_col)\n",
    "    ax.axhline(0)\n",
    "    ax.axvline(0)\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(1, 3, figsize=(12, 4))\n",
    "scatterplot_pc(df_pca_merged,x_col=\"PC1\",y_col = \"PC2\", ax = axs[0],expl_var = round(delta_explained_variance[0]+delta_explained_variance[1],3)*100)\n",
    "scatterplot_pc(df_pca_merged,x_col=\"PC1\",y_col = \"PC3\", ax = axs[1],expl_var = round(delta_explained_variance[0]+delta_explained_variance[2],3)*100)\n",
    "scatterplot_pc(df_pca_merged,x_col=\"PC2\",y_col = \"PC3\", ax = axs[2],expl_var = round(delta_explained_variance[1]+delta_explained_variance[2],3)*100)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By plotting the 3 principal planes obtained labeled with the categorical variables, interesting relations were found. This relations could help us to give names to each PC.\n",
    "\n",
    "- **PC1 (Wellness):** Can be interpreted as a person wellness because it makes a good separation between having or not a sleep disorder and also of having or not overweight.\n",
    "\n",
    "- **PC2 (Stress Level):** Higher values of the PC2 indicates higher stress levels (The stress level is well organized in this component). Also, positive values of this component are related with a greater age and having high pressure, while lower values are related with a more active life.\n",
    "\n",
    "- **PC3 (Physical Activity):** In this component a relation is less clear  with any of the categorical variables, so the interpretation from the continous variables contribution is mantained as the PC name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for categorical_variable in categorical_variables:\n",
    "    fig, axs = plt.subplots(1, 4, figsize=(15, 4), gridspec_kw={'width_ratios': [1, 1, 1, 0.2]})\n",
    "    \n",
    "    # Create the scatter plots\n",
    "    scatter1 = scatterplot_pc(\n",
    "        df_pca_merged.sort_values(by=[categorical_variable]),\n",
    "        x_col=\"PC1\",\n",
    "        y_col=\"PC2\",\n",
    "        hue_col=categorical_variable,\n",
    "        ax=axs[0],\n",
    "        expl_var=round(delta_explained_variance[0] + delta_explained_variance[1], 3)*100\n",
    "    )\n",
    "    axs[0].legend().set_visible(False)\n",
    "\n",
    "    \n",
    "    scatterplot_pc(\n",
    "        df_pca_merged.sort_values(by=[categorical_variable]),\n",
    "        x_col=\"PC1\",\n",
    "        y_col=\"PC3\",\n",
    "        hue_col=categorical_variable,\n",
    "        ax=axs[1],\n",
    "        expl_var=round(delta_explained_variance[0] + delta_explained_variance[2], 3)*100\n",
    "    )\n",
    "    axs[1].legend().set_visible(False) \n",
    "\n",
    "    \n",
    "    scatterplot_pc(\n",
    "        df_pca_merged.sort_values(by=[categorical_variable]),\n",
    "        x_col=\"PC2\",\n",
    "        y_col=\"PC3\",\n",
    "        hue_col=categorical_variable,\n",
    "        ax=axs[2],\n",
    "        expl_var=round(delta_explained_variance[1] + delta_explained_variance[2], 3)*100\n",
    "    )\n",
    "    axs[2].legend().set_visible(False)\n",
    "\n",
    "    \n",
    "    # Add Manual Legend to side\n",
    "    handles, labels = axs[0].get_legend_handles_labels()\n",
    "    axs[3].legend(handles=handles, labels=labels, loc='center', frameon=False)\n",
    "    axs[3].axis('off') \n",
    "\n",
    "    # plot\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A first glance at possible meaning behind the principal components is only found when plotting the transformed data in three dimensions with a colorcoding for the categorical features. Doing so, first patterns emerge. Some observable pattern include: \n",
    "- male data points being closer to the origin and female values data points\n",
    "- clusters of occupations visible, sometimes in combination with stress level like two clusters for nurses, one with low and one with high stress level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scatterplot_3d(ax, df, x_col, y_col, z_col, hue_col):\n",
    "    \n",
    "    unique_hues = df[hue_col].unique()\n",
    "    for hue in unique_hues:\n",
    "        hue_df = df[df[hue_col] == hue]\n",
    "        ax.scatter(hue_df[x_col], hue_df[y_col], hue_df[z_col], label=hue)\n",
    "\n",
    "    # cosmetics\n",
    "    ax.set_xlabel(x_col)\n",
    "    ax.set_ylabel(y_col)\n",
    "    ax.set_zlabel(z_col)\n",
    "    ax.set_title(f'PC 1-3 with hue: {hue_col}')\n",
    "    ax.legend(title=hue_col)\n",
    "\n",
    "def scatterplot_3d_subplots(df, categorical_variables, x_col, y_col, z_col):\n",
    "    \n",
    "    n_cols = 3\n",
    "    n_rows = (len(categorical_variables) + 1) // n_cols\n",
    "    fig = plt.figure(figsize=(15,10))\n",
    "\n",
    "    # make subplots    \n",
    "    for i, categorical_variable in enumerate(categorical_variables):\n",
    "        ax = fig.add_subplot(n_rows, n_cols, i + 1, projection='3d')\n",
    "        scatterplot_3d(ax, df.sort_values(by=[categorical_variable]), x_col, y_col, z_col, categorical_variable)\n",
    "    \n",
    "    # cosmetics\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Call the function to create subplots\n",
    "scatterplot_3d_subplots(df_pca_merged, categorical_variables, x_col=\"PC1\", y_col=\"PC2\", z_col=\"PC3\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These emerging patterns between the principal components, the numerical and categorical variables and more specifically sleep disorder show an apparent clustering potential that can be further investigated in the second part of this project via distance based metrics. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stability of the Principal components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PCAcovstability(X):\n",
    "    #create H matrix to do covariance for pca\n",
    "    n, p = X.shape\n",
    "    H = np.eye(n) - np.ones((n, n)) / n\n",
    "    X = np.dot(H, X)\n",
    "    S = np.cov(X, rowvar=False, bias=True) \n",
    "    \n",
    "    T, D = eigsort(S)\n",
    "    Y = np.dot(X, T) \n",
    "    \n",
    "    for j in range(p):\n",
    "        if T[0, j] < 0:\n",
    "            T[:, j] = -T[:, j]\n",
    "    \n",
    "    v = np.zeros((n, n))\n",
    "    for i in range(n):\n",
    "        Xi = np.delete(X, i, axis=0)  \n",
    "        Si = np.cov(Xi, rowvar=False, bias=True)\n",
    "        Ti, Di = eigsort(Si)\n",
    "        \n",
    "        for j in range(p):\n",
    "            if Ti[0, j] < 0:\n",
    "                Ti[:, j] = -Ti[:, j]\n",
    "        \n",
    "        Yi = np.dot(Xi, Ti)\n",
    "        y = X[i, :] @ Ti \n",
    "        \n",
    "        Yplusi = np.vstack([Yi[:i, :], y, Yi[i:, :]])\n",
    "        \n",
    "       #calculate euclidean distances \n",
    "        for k in range(n):\n",
    "            v[k, i] = np.linalg.norm(Yplusi[k, :] - Y[k, :])\n",
    "    \n",
    "    v = np.sqrt(v) \n",
    "    #save summarize values in table to analysis\n",
    "    Rad = np.zeros((n, 6))\n",
    "    for i in range(n):\n",
    "        Rad[i, 0] = np.min(v[i, :])\n",
    "        Rad[i, 1] = np.max(v[i, :])\n",
    "        Rad[i, 2] = np.quantile(v[i, :], 0.50)\n",
    "        Rad[i, 3] = np.quantile(v[i, :], 0.75)\n",
    "        Rad[i, 4] = np.quantile(v[i, :], 0.90)\n",
    "        Rad[i, 5] = np.quantile(v[i, :], 0.95)\n",
    "    \n",
    "    rowsummary = np.vstack([np.mean(Rad, axis=0), np.std(Rad, axis=0), np.median(Rad, axis=0), np.median(np.abs(Rad - np.median(Rad, axis=0)), axis=0)])\n",
    "    \n",
    "    stats_labels = ['Mean', 'Standard Deviation', 'Median', 'Mean Absolute Deviation (MAD)']\n",
    "    pca_labels = ['PCA 1', 'PCA 2', 'PCA 3', 'PCA 4', 'PCA 5', 'PCA 6']\n",
    "    \n",
    "    rowsummary_df = pd.DataFrame(rowsummary.T, columns=stats_labels, index=pca_labels)\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 1, figsize=(10, 5))\n",
    "\n",
    "    # Boxplot\n",
    "    axes[0].boxplot(v)\n",
    "    axes[0].set_title('Euclidean distance of each unit among configurations')\n",
    "\n",
    "    # Barplot\n",
    "    euclidean_distances = np.mean(v, axis=1)\n",
    "    axes[1].bar(range(1, n + 1), euclidean_distances, color='orange', alpha=0.7)\n",
    "    axes[1].set_xlabel('Observation Index')\n",
    "    axes[1].set_ylabel('Euclidean Distance')\n",
    "    axes[1].set_title('PCA Configuration Stability')\n",
    "    axes[1].grid()\n",
    "\n",
    "    # Show the plot\n",
    "    plt.tight_layout()  # Adjust spacing\n",
    "    plt.show()  \n",
    "         \n",
    "    return rowsummary_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To perform the stability analysis using the leave-one-out method, we obtained 354 different outputs, which makes it challenging to interpret the results visually. Even though you can see that the values are very consistence across both graph.\n",
    "\n",
    "The values in the table are quite consistent across the different PCAs, indicating that they all exhibit similar average Euclidean distances and very low standard deviations. This suggests that the principal components are stable, remaining unaffected by changes in the data. Consequently, this implies that the PCA captures the underlying structure of the data reliably, providing confidence in the robustness of the results. In general, this suggests that the PCA model is likely to generalize well to new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PCAcovstability(data_standardized_numeric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion Part 1\n",
    " \n",
    "In the first Part of this project work, an exploratory data analysis has been performed in support of a principal component analysis. The principal component analysis has been shown to deliver three principal components that explain the datas variance well and are stable when genaralizing to new data, while attributing intuitive characteristics to the new dimensions has shown to be challanging. Only when visualizing the categorical variables together with the data in the new dimensions, patterns and cluster emerge. To further understand the in this analysis discovered patterns will be the task of the second part of this work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Multidimensional Scaling (MDS) and Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distance Matrix: mahalanobis\n",
    "\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import mahalanobis\n",
    "\n",
    "def maha_dist(data,numeric_variables):\n",
    "    data_numeric = data[numeric_variables]\n",
    "\n",
    "    # Step 1: Compute the covariance matrix and its inverse\n",
    "    cov_matrix = np.cov(data_numeric.T)  # Transpose because rows are observations\n",
    "    inv_cov_matrix = np.linalg.inv(cov_matrix)\n",
    "\n",
    "    # Step 2: Create an empty distance matrix\n",
    "    n = len(data_numeric)\n",
    "    m_distance_mahalanobis = np.zeros((n, n))\n",
    "\n",
    "    # Step 3: Compute pairwise Mahalanobis distances\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            xi = data_numeric.iloc[i].values\n",
    "            xj = data_numeric.iloc[j].values\n",
    "            m_distance_mahalanobis[i, j] = mahalanobis(xi, xj, inv_cov_matrix)\n",
    "\n",
    "    # Step 4: Convert distance matrix to a DataFrame for better readability\n",
    "    df_distance_mahalanobis = pd.DataFrame(m_distance_mahalanobis, index=data_numeric.index, columns=data_numeric.index)\n",
    "    return (m_distance_mahalanobis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maha_dist(data_standardized_numeric, numeric_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import gower\n",
    "\n",
    "# Compute Gower similarity between rows\n",
    "similarity_matrix = gower.gower_matrix(data)\n",
    "m_distance_gowers = 1 - similarity_matrix\n",
    "df_distance_gowers = pd.DataFrame(m_distance_gowers, index=data.index, columns=data.index)\n",
    "df_distance_gowers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multidimensional Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Distance Matrix -> distance measure\n",
    "- pot force to fullfill euclidean property -> slides 5\n",
    "- principal coordinates -> slide 6\n",
    "- check euclidian property !!!\n",
    "opt: \n",
    "- compare MDS configurations - split in groups (male/female) -> only when fulfilling equal geometric variability -> slide 10\n",
    "- correlations: original variables vs first n principal coordinates heatmap\n",
    "- group individuals via mds scatterplot with categorical colorcoding\n",
    "- Gowers interpolation formula - > mds stability & variables partial influence on principal coordinates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Interpretation of principal coordinates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usual K-means\n",
    " - Non-hierarchical\n",
    " - Minimize the WCSS (within-clusters sum of squeares)\n",
    " - Euclidean distance only for uncorrelated numerical variables.\n",
    "\n",
    " ### selection of number of clusters\n",
    " - Average silhouette\n",
    "     Calulates the ratio between the difference in the distance of the selected group and the closest group and the max of the two of them.\n",
    "     Near 1 is correctly classified, near to -1 should have been clasified in the neighborhood cluster. Near to 0, any of them.\n",
    "     The greater the average silhouette the better the global quality of the clustering.\n",
    " - Elbow method \n",
    "     For large datasets. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "X_pca = pca.fit_transform(data_pca)\n",
    "\n",
    "# Elbow Method\n",
    "wcss = []\n",
    "for k in range(1, 11):  # Try k from 1 to 10\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    kmeans.fit(X_pca)\n",
    "    wcss.append(kmeans.inertia_)  # inertia_ is the WCSS\n",
    "\n",
    "# Plotting the Elbow Graph\n",
    "plt.plot(range(1, 11), wcss)\n",
    "plt.title('Elbow Method for Optimal k')\n",
    "plt.xlabel('Number of Clusters (k)')\n",
    "plt.ylabel('WCSS')\n",
    "plt.show()\n",
    "\n",
    "# 2, 5 or 7 clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "silhouette_scores = []\n",
    "for k in range(2, 11):  # Silhouette score is undefined for k=1\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    kmeans.fit(X_pca)\n",
    "    score = silhouette_score(X_pca, kmeans.labels_)\n",
    "    silhouette_scores.append(score)\n",
    "\n",
    "# Plotting the Silhouette Scores\n",
    "plt.plot(range(2, 11), silhouette_scores)\n",
    "plt.title('Silhouette Method for Optimal k')\n",
    "plt.xlabel('Number of Clusters (k)')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Perform K-Means clustering on the reduced data\n",
    "kmeans = KMeans(n_clusters=2, random_state=14, init=\"random\", n_init=30)\n",
    "kmeans.fit(X_pca)\n",
    "\n",
    "# Get the cluster labels\n",
    "labels = kmeans.labels_\n",
    "\n",
    "# Step 3: Visualize the results\n",
    "#cmap='viridis'\n",
    "\n",
    "fig, axs = plt.subplots(1, 3, figsize=(12, 4))\n",
    "scatterplot_pc(df_pca_merged,x_col=\"PC1\",y_col = \"PC2\", hue_col=labels,\n",
    "                ax = axs[0],expl_var = round(delta_explained_variance[0]+delta_explained_variance[1],3)*100)\n",
    "scatterplot_pc(df_pca_merged,x_col=\"PC1\",y_col = \"PC3\", hue_col=labels,\n",
    "                ax = axs[1],expl_var = round(delta_explained_variance[0]+delta_explained_variance[2],3)*100)\n",
    "scatterplot_pc(df_pca_merged,x_col=\"PC2\",y_col = \"PC3\", hue_col=labels,\n",
    "                ax = axs[2],expl_var = round(delta_explained_variance[1]+delta_explained_variance[2],3)*100)\n",
    "plt.show()\n",
    "\n",
    "# CHECKING THE CLUSTERS\n",
    "# Get the inertia (WCSS)\n",
    "inertia = kmeans.inertia_\n",
    "print(f\"Inertia (WCSS): {inertia}\")\n",
    "\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Assuming kmeans.labels_ contains the cluster labels\n",
    "sil_score = silhouette_score(X_pca, kmeans.labels_)\n",
    "print(f\"Silhouette Score: {sil_score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Perform K-Means clustering on the reduced data\n",
    "kmeans5 = KMeans(n_clusters=5, random_state=14, init=\"random\", n_init=30)\n",
    "kmeans5.fit(X_pca)\n",
    "\n",
    "# Get the cluster labels\n",
    "labels = kmeans5.labels_\n",
    "\n",
    "# Step 3: Visualize the results\n",
    "\n",
    "fig, axs = plt.subplots(1, 3, figsize=(12, 4))\n",
    "scatterplot_pc(df_pca_merged,x_col=\"PC1\",y_col = \"PC2\", hue_col=labels,\n",
    "                ax = axs[0],expl_var = round(delta_explained_variance[0]+delta_explained_variance[1],3)*100)\n",
    "scatterplot_pc(df_pca_merged,x_col=\"PC1\",y_col = \"PC3\", hue_col=labels,\n",
    "                ax = axs[1],expl_var = round(delta_explained_variance[0]+delta_explained_variance[2],3)*100)\n",
    "scatterplot_pc(df_pca_merged,x_col=\"PC2\",y_col = \"PC3\", hue_col=labels,\n",
    "                ax = axs[2],expl_var = round(delta_explained_variance[1]+delta_explained_variance[2],3)*100)\n",
    "plt.show()\n",
    "\n",
    "# Get the inertia (WCSS)\n",
    "inertia5 = kmeans5.inertia_\n",
    "print(f\"Inertia (WCSS): {inertia5}\")\n",
    "\n",
    "# Assuming kmeans.labels_ contains the cluster labels\n",
    "sil_score = silhouette_score(X_pca, kmeans5.labels_)\n",
    "print(f\"Silhouette Score: {sil_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Perform K-Means clustering on the reduced data\n",
    "kmeans7 = KMeans(n_clusters=7, random_state=14, init=\"random\", n_init=30)\n",
    "kmeans7.fit(X_pca)\n",
    "\n",
    "# Get the cluster labels\n",
    "labels = kmeans7.labels_\n",
    "\n",
    "# Step 3: Visualize the results\n",
    "\n",
    "fig, axs = plt.subplots(1, 3, figsize=(12, 4))\n",
    "scatterplot_pc(df_pca_merged,x_col=\"PC1\",y_col = \"PC2\", hue_col=labels,\n",
    "                ax = axs[0],expl_var = round(delta_explained_variance[0]+delta_explained_variance[1],3)*100)\n",
    "scatterplot_pc(df_pca_merged,x_col=\"PC1\",y_col = \"PC3\", hue_col=labels,\n",
    "                ax = axs[1],expl_var = round(delta_explained_variance[0]+delta_explained_variance[2],3)*100)\n",
    "scatterplot_pc(df_pca_merged,x_col=\"PC2\",y_col = \"PC3\", hue_col=labels,\n",
    "                ax = axs[2],expl_var = round(delta_explained_variance[1]+delta_explained_variance[2],3)*100)\n",
    "plt.show()\n",
    "\n",
    "# Get the inertia (WCSS)\n",
    "inertia7 = kmeans7.inertia_\n",
    "print(f\"Inertia (WCSS): {inertia7}\")\n",
    "\n",
    "# Assuming kmeans.labels_ contains the cluster labels\n",
    "sil_score = silhouette_score(X_pca, kmeans7.labels_)\n",
    "print(f\"Silhouette Score: {sil_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pca[\"pulse_pressure\"] = data_pca['blood_pressure_systolic']-data_pca['blood_pressure_diastolic']\n",
    "data_kmeans = data_pca.drop(['blood_pressure_diastolic', 'blood_pressure_systolic'], axis=1)\n",
    "data_kmeans.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elbow Method\n",
    "wcss = []\n",
    "for k in range(1, 11):  # Try k from 1 to 10\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    kmeans.fit(data_kmeans)\n",
    "    wcss.append(kmeans.inertia_)  # inertia_ is the WCSS\n",
    "\n",
    "# Plotting the Elbow Graph\n",
    "plt.plot(range(1, 11), wcss)\n",
    "plt.title('Elbow Method for Optimal k')\n",
    "plt.xlabel('Number of Clusters (k)')\n",
    "plt.ylabel('WCSS')\n",
    "plt.show()\n",
    "\n",
    "# 2, 5 or 7 cluster\n",
    "#_---------------------------------------------------------------------\n",
    "\n",
    "silhouette_scores = []\n",
    "for k in range(2, 11):  # Silhouette score is undefined for k=1\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    kmeans.fit(data_kmeans)\n",
    "    score = silhouette_score(data_kmeans, kmeans.labels_)\n",
    "    silhouette_scores.append(score)\n",
    "\n",
    "# Plotting the Silhouette Scores\n",
    "plt.plot(range(2, 11), silhouette_scores)\n",
    "plt.title('Silhouette Method for Optimal k')\n",
    "plt.xlabel('Number of Clusters (k)')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Perform K-Means clustering on the reduced data\n",
    "kmeans = KMeans(n_clusters=2, random_state=14, init=\"random\", n_init=30)\n",
    "kmeans.fit(data_kmeans)\n",
    "\n",
    "# Add cluster labels to the DataFrame\n",
    "data_kmeans['Cluster'] = kmeans.labels_\n",
    "\n",
    "# Create pairplot to visualize the clusters\n",
    "sns.pairplot(data_kmeans, hue='Cluster', palette='Dark2')\n",
    "plt.title('Pairwise Plot of Clusters')\n",
    "plt.show()\n",
    "\n",
    "# CHECKING THE CLUSTERS\n",
    "# Get the inertia (WCSS)\n",
    "inertia = kmeans.inertia_\n",
    "print(f\"Inertia (WCSS): {inertia}\")\n",
    "\n",
    "sil_score = silhouette_score(data_kmeans, kmeans.labels_)\n",
    "print(f\"Silhouette Score: {sil_score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Perform K-Means clustering on the reduced data\n",
    "kmeans = KMeans(n_clusters=5, random_state=14, init=\"random\", n_init=30)\n",
    "kmeans.fit(data_kmeans)\n",
    "\n",
    "# Add cluster labels to the DataFrame\n",
    "data_kmeans['Cluster'] = kmeans.labels_\n",
    "\n",
    "# Create pairplot to visualize the clusters\n",
    "sns.pairplot(data_kmeans, hue='Cluster', palette='Dark2')\n",
    "plt.title('Pairwise Plot of Clusters')\n",
    "plt.show()\n",
    "\n",
    "# CHECKING THE CLUSTERS\n",
    "# Get the inertia (WCSS)\n",
    "inertia = kmeans.inertia_\n",
    "print(f\"Inertia (WCSS): {inertia}\")\n",
    "\n",
    "sil_score = silhouette_score(data_kmeans, kmeans.labels_)\n",
    "print(f\"Silhouette Score: {sil_score}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As alternative to Principal Component Analysis, Multi Dimensional Scaling (MDS) offers a distance based dimension reduction method. Input to MDS are a distance (or dissimilarity) Matrix generated by a chosen distance. This distance has to fulfill the following properties:\n",
    "\n",
    "- Symmetry (of distance between two points, A->B = B->A)\n",
    "- Non-Negativity (distance between two points must be non-negative)\n",
    "- Identity of Indiscernibles (distance A->B is only zero if A=B)\n",
    "- Triangular Inequality (A->C <= A->B->C)\n",
    "\n",
    "in the following, we utilize the Mahalanobis distance and the Gower dissimilarity to set up two distance matrices, one of which will then used to apply MDS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_mds = data[numeric_variables+categorical_variables]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distance Matrix: mahalanobis\n",
    "\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import mahalanobis\n",
    "\n",
    "def maha_dist(data,\n",
    "              numeric_variables):\n",
    "    data_numeric = data[numeric_variables]\n",
    "\n",
    "    # covariance matrix + inverse\n",
    "    cov_matrix = np.cov(data_numeric.T)  # Transpose because rows are observations\n",
    "    inv_cov_matrix = np.linalg.inv(cov_matrix)\n",
    "\n",
    "    # empty distance matrix\n",
    "    n = len(data_numeric)\n",
    "    m_distance_mahalanobis = np.zeros((n, n))\n",
    "\n",
    "    # pairwise Mahalanobis distances\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            xi = data_numeric.iloc[i].values\n",
    "            xj = data_numeric.iloc[j].values\n",
    "            m_distance_mahalanobis[i, j] = mahalanobis(xi, xj, inv_cov_matrix)\n",
    "\n",
    "    return m_distance_mahalanobis\n",
    "\n",
    "m_distance_mahalanobis = maha_dist(data_mds,numeric_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distance Matrix: Gowers\n",
    "import gower\n",
    "m_distance_gowers = gower.gower_matrix(data_mds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the corresponding distance matrix is not possible, as the dataset with 374 data points is simply too big. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plot distance Matrices WARNING THIS TAKES A LONG TIME TO RUN\n",
    "# fig, axes = plt.subplots(1, 2, figsize=(160, 60))  # Adjust size as needed\n",
    "\n",
    "# # Plot the first heatmap\n",
    "# sns.heatmap(m_distance_mahalanobis, annot=True, fmt=\".1f\", cmap=\"coolwarm\", cbar=True, ax=axes[0])\n",
    "# axes[0].set_title(\"Mahalanobis Distance Matrix\")\n",
    "# axes[0].set_xlabel(\"Points\")\n",
    "# axes[0].set_ylabel(\"Points\")\n",
    "\n",
    "# # Plot the second heatmap\n",
    "# sns.heatmap(m_distance_gowers, annot=True, fmt=\".1f\", cmap=\"coolwarm\", cbar=True, ax=axes[1])\n",
    "# axes[1].set_title(\"Gower Dissimilarity Matrix\")\n",
    "# axes[1].set_xlabel(\"Points\")\n",
    "# axes[1].set_ylabel(\"Points\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the distance/dissimilarity Matrices set up, the MDS is performed using Gowers dissimilarity matrix as it also contains the categorical/binary that can be found in the dataset data, whereas Mahalanobis distance was only created with the numerical variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import MDS\n",
    "\n",
    "def coorp(D):\n",
    "\n",
    "    n = D.shape[0]\n",
    "    # Centering matrix & double centering distance matrix\n",
    "    H = np.eye(n) - np.ones((n, n)) / n\n",
    "    B = -0.5 * H @ D @ H\n",
    "\n",
    "    # Ensure the matrix is positive semidefinite\n",
    "    eigenvalues = np.linalg.eigvalsh(B)\n",
    "    epsilon = 1e-6\n",
    "    if np.min(eigenvalues) < -epsilon:\n",
    "        raise ValueError(\"Matrix D is not Euclidean and needs preprocessing.\")\n",
    "\n",
    "    # Compute eigenvalues and eigenvectors\n",
    "    eigvals, eigvecs = np.linalg.eigh(B)\n",
    "    eigvals = eigvals[::-1]  # Reverse to descending order\n",
    "    eigvecs = eigvecs[:, ::-1]\n",
    "\n",
    "    # Filter non-zero eigenvalues\n",
    "    positive_eigvals = eigvals > epsilon\n",
    "    vaps = eigvals[positive_eigvals]\n",
    "    principal_components = eigvecs[:, positive_eigvals]\n",
    "\n",
    "    # Compute principal coordinates\n",
    "    Y = principal_components * np.sqrt(vaps)\n",
    "\n",
    "    # Variance explained\n",
    "    percent = (vaps / np.sum(vaps)) * 100\n",
    "    acum = np.cumsum(percent)\n",
    "\n",
    "    return Y, vaps, percent, acum\n",
    "\n",
    "Y, vaps, percent, acum = coorp(m_distance_gowers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having performed the MDS, as a first step the cumulative explained variance is plotted to show how many Principal Coordinates are required to represent the full Variance of the Data Set. The plot below shows, how 80 % of the variance can be covered with 7 Principal Components. Notable is how from the 7th principal coordinate onwards, the added variance decreases significantly as the cumulative variance goes towards 100 % "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Plot cumulative variance explained\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(acum, marker='o', linestyle='-', color='b', linewidth=2)\n",
    "plt.xlabel(\"Eigenvalues\", fontsize=12)\n",
    "plt.ylabel(\"Cumulative Variance (%)\", fontsize=12)\n",
    "plt.title(\"Cumulative Variance Explained\", fontsize=14)\n",
    "plt.axvline(x=7, color='red', linestyle='--', label=\"Vertical line\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the first three principal coordinates, does not give significant insight into how these principal coordinates can be interpreted. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for i in range(0,3):\n",
    "\n",
    "#n  components\n",
    "n_coords = 3\n",
    "fig, axes = plt.subplots(1, n_coords, figsize=(15, 5), sharey=True)\n",
    "\n",
    "axes[0].scatter(Y[:, 0], Y[:, 1], c='b', marker='o', label='Points')\n",
    "axes[0].set_xlabel(f\"Principal Coordinate {1}\", fontsize=12)\n",
    "axes[0].set_ylabel(f\"Principal Coordinate {2}\", fontsize=12)\n",
    "axes[0].set_title(f\"Variance Explained: {acum[1]:.2f}%\", fontsize=14)\n",
    "axes[0].grid()\n",
    "\n",
    "axes[1].scatter(Y[:, 0], Y[:, 2], c='b', marker='o', label='Points')\n",
    "axes[1].set_xlabel(f\"Principal Coordinate {1}\", fontsize=12)\n",
    "axes[1].set_ylabel(f\"Principal Coordinate {3}\", fontsize=12)\n",
    "axes[1].set_title(f\"Variance Explained: {acum[0]+(acum[2]-acum[1]):.2f}%\", fontsize=14)\n",
    "axes[1].grid()\n",
    "\n",
    "axes[2].scatter(Y[:, 1], Y[:, 2], c='b', marker='o', label='Points')\n",
    "axes[2].set_xlabel(f\"Principal Coordinate {2}\", fontsize=12)\n",
    "axes[2].set_ylabel(f\"Principal Coordinate {3}\", fontsize=12)\n",
    "axes[2].set_title(f\"Variance Explained: {acum[2]-acum[0]:.2f}%\", fontsize=14)\n",
    "axes[2].grid()\n",
    "\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To gain a deeper understanding on how to interprete the principal coordinates, the same plot is done with a color coding of the categorical and the binary Variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_mds_output = data_mds.copy()\n",
    "data_mds_output[[\"MDS1\",\"MDS2\",\"MDS3\"]] = Y[:,0:3]\n",
    "\n",
    "for i, var in enumerate(categorical_variables):\n",
    "    fig, axes = plt.subplots(1, 4, figsize=(15, 5), gridspec_kw={'width_ratios': [1, 1, 1, 0.2]})\n",
    "    \n",
    "    # MDS1 vs MDS2\n",
    "    sns.scatterplot(\n",
    "        ax=axes[0],\n",
    "        data=data_mds_output,\n",
    "        x='MDS1',\n",
    "        y='MDS2',\n",
    "        hue=var,\n",
    "        palette='Set1',\n",
    "        s=100,\n",
    "        edgecolor='black'\n",
    "    )\n",
    "    axes[0].set_title(f'MDS plot colored by {var},\\n Variance Explained: {acum[1]:.2f}%')\n",
    "    axes[0].legend().set_visible(False) \n",
    "\n",
    "    \n",
    "    # MDS1 vs MDS3\n",
    "    sns.scatterplot(\n",
    "        ax=axes[1],\n",
    "        data=data_mds_output,\n",
    "        x='MDS1',\n",
    "        y='MDS3',\n",
    "        hue=var,\n",
    "        palette='Set1',\n",
    "        s=100,\n",
    "        edgecolor='black'\n",
    "    )\n",
    "    axes[1].set_title(f'MDS plot colored by {var},\\n Variance Explained: {acum[0] + (acum[2] - acum[1]):.2f}%')\n",
    "    axes[1].legend().set_visible(False) \n",
    "\n",
    "    # MDS2 vs MDS3\n",
    "    sns.scatterplot(\n",
    "        ax=axes[2],\n",
    "        data=data_mds_output,\n",
    "        x='MDS2',\n",
    "        y='MDS3',\n",
    "        hue=var,\n",
    "        palette='Set1',\n",
    "        s=100,\n",
    "        edgecolor='black'\n",
    "    )\n",
    "    axes[2].set_title(f'MDS plot colored by {var},\\n Variance Explained: {acum[2] - acum[0]:.2f}% ')\n",
    "    axes[2].legend().set_visible(False) \n",
    "\n",
    "    # legend\n",
    "    handles, labels = axes[0].get_legend_handles_labels()\n",
    "    axes[3].legend(handles=handles, labels=labels, loc='center', frameon=False)\n",
    "    axes[3].axis('off') \n",
    "\n",
    "    axes[0].axhline(0)\n",
    "    axes[0].axvline(0)\n",
    "\n",
    "    axes[1].axhline(0)\n",
    "    axes[1].axvline(0)\n",
    "\n",
    "    axes[2].axhline(0)\n",
    "    axes[2].axvline(0)\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correlations between original Variables and first three Principal Coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr, spearmanr\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "# Cramers V\n",
    "def cramer_v(u, X):\n",
    "    n = len(u)\n",
    "    percentiles = np.percentile(X, [0, 25, 50, 75, 100])\n",
    "    Xcat = np.ones(n)\n",
    "    Xcat[(X > percentiles[1]) & (X <= percentiles[2])] = 2\n",
    "    Xcat[(X > percentiles[2]) & (X <= percentiles[3])] = 3\n",
    "    Xcat[X > percentiles[3]] = 4\n",
    "    \n",
    "    # contingency table\n",
    "    cont_table = pd.crosstab(u, Xcat)\n",
    "    chi2, p, dof, expected = chi2_contingency(cont_table)\n",
    "    \n",
    "    k = len(np.unique(u))\n",
    "    r = len(np.unique(Xcat))\n",
    "    \n",
    "    V = np.sqrt(chi2 / (n * min(k - 1, r - 1)))\n",
    "    return V\n",
    "\n",
    "# Main correlation function\n",
    "def correlaciones2(X, Y, pcuant, pnominal):\n",
    "    collumns = X.columns\n",
    "    \n",
    "    X = X.to_numpy()\n",
    "    p = X.shape[1]\n",
    "    pordinal = p - pcuant - pnominal\n",
    "    corr_table = np.zeros((p, 3))\n",
    "\n",
    "    # quantitative variables\n",
    "    for i in range(pcuant):\n",
    "        corr_table[i, :] = [pearsonr(Y[:, 0], X[:, i])[0], pearsonr(Y[:, 1], X[:, i])[0], pearsonr(Y[:, 2], X[:, i])[0]]\n",
    "\n",
    "    # nominal variables\n",
    "    for i in range(pcuant, pcuant + pnominal):\n",
    "        corr_table[i, :] = [cramer_v(X[:, i], Y[:, 0]), cramer_v(X[:, i], Y[:, 1]), cramer_v(X[:, i], Y[:, 2])]\n",
    "\n",
    "    # ordinal variables\n",
    "    for i in range(pcuant + pnominal, p):\n",
    "        corr_table[i, :] = [spearmanr(Y[:, 0], X[:, i])[0], spearmanr(Y[:, 1], X[:, i])[0], spearmanr(Y[:, 2], X[:, i])[0]]\n",
    "\n",
    "    # Plot\n",
    "    L1 = ['PC1', 'PC2', 'PC3']    \n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    cmap = sns.diverging_palette(250, 15, as_cmap=True)\n",
    "    cax = ax.imshow(corr_table, cmap=cmap, vmin=-1, vmax=1)\n",
    "\n",
    "    ax.set_xticks(np.arange(3))\n",
    "    ax.set_xticklabels(L1)\n",
    "\n",
    "    ax.set_yticks(np.arange(p))  # Set numeric tick positions\n",
    "    ax.set_yticklabels(collumns)  # Use the actual column names as labels\n",
    "\n",
    "    ax.set_title('Principal Coordinates Heatmap', fontsize=12)\n",
    "    fig.colorbar(cax, ax=ax)\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "    return corr_table\n",
    "\n",
    "\n",
    "\n",
    "# excecute\n",
    "corr_table = correlaciones2(X = data_mds[numeric_variables + categorical_variables],#.to_numpy(), \n",
    "               Y = Y[:,0:3], \n",
    "               pcuant = 7, \n",
    "               pnominal = 4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Influence of Quantitative and Categorical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import pdist, squareform\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "\n",
    "\n",
    "def influence(X, p1, p2):\n",
    "    columns = X.columns\n",
    "    X = X.to_numpy()\n",
    "\n",
    "    n, p = X.shape\n",
    "    p3 = p - p1 - p2\n",
    "\n",
    "    # Compute distance matrix\n",
    "    if p2 + p3 == 0:\n",
    "        D = squareform(pdist(X, metric='mahalanobis'))\n",
    "        D2 = D ** 2\n",
    "    else:\n",
    "        D2 = gower.gower_matrix(data)\n",
    "\n",
    "\n",
    "    # Principal coordinates\n",
    "    Y, vaps, _, _ = coorp(D2)\n",
    "    v = 1.0 / vaps[:Y.shape[1]]\n",
    "    Lambda = np.diag(v)\n",
    "    g = np.sum(Y ** 2, axis=1)\n",
    "\n",
    "    m0 = np.min(X, axis=0)\n",
    "    m1 = np.max(X, axis=0)\n",
    "\n",
    "    # evaluate Influence of continious variables \n",
    "    for k in range(p1):\n",
    "        step = (m1[k] - m0[k]) / 50\n",
    "        Xnew = np.arange(m0[k], m1[k] + step, step).reshape(-1, 1)\n",
    "        nXnew = Xnew.shape[0]\n",
    "        virtual = np.zeros((nXnew, p))\n",
    "        virtual[:, k] = Xnew[:, 0]\n",
    "\n",
    "        dnew = np.zeros((nXnew, n))\n",
    "        for i in range(nXnew):\n",
    "            Xvirtual = np.vstack([X, virtual[i, :]])\n",
    "            D2new = gower.gower_matrix(Xvirtual)\n",
    "            dnew[i, :] = D2new[-1, :-1]\n",
    "\n",
    "        ynew = np.zeros((nXnew, Y.shape[1]))\n",
    "        for i in range(nXnew):\n",
    "            ynew[i, :] = 0.5 * (g - dnew[i, :]) @ Y @ Lambda\n",
    "\n",
    "        # plot\n",
    "        plt.figure(1,figsize=(10,5))\n",
    "        plt.plot(ynew[:, 0], ynew[:, 1],linestyle=':', label=f'{columns[k]}', marker='o')\n",
    "        plt.legend(title=\"Continuous Variables\", fontsize=10)\n",
    "        plt.title(\"Influence of Quantitative Variables (Gower)\", fontsize=12)\n",
    "        plt.xlabel('Coordinate 1')\n",
    "        plt.ylabel('Coordinate 2')\n",
    "        plt.grid()\n",
    "\n",
    "    # evaluate Influence of categorical/binary variables \n",
    "    plt.figure(figsize=(10,5))\n",
    "    colors = cm.get_cmap('tab10') \n",
    "    for k in range(p1, p):\n",
    "\n",
    "        # principalcoordinates\n",
    "        Xnew = np.unique(X[:, k]) \n",
    "        nXnew = len(Xnew)\n",
    "        virtual = np.zeros((nXnew, p))\n",
    "        virtual[:, k] = Xnew\n",
    "        dnew = np.zeros((nXnew, n))\n",
    "        \n",
    "        for i in range(nXnew):\n",
    "            Xvirtual = np.vstack([X, virtual[i, :]]) \n",
    "            D2new = gower.gower_matrix(Xvirtual)  \n",
    "            dnew[i, :] = D2new[-1, :-1]\n",
    "\n",
    "        ynew = np.zeros((nXnew, Y.shape[1]))\n",
    "        for i in range(nXnew):\n",
    "            ynew[i, :] = 0.5 * (g - dnew[i, :]) @ Y @ Lambda\n",
    "\n",
    "        # plot\n",
    "        plt.plot(\n",
    "            ynew[:, 0], ynew[:, 1], linestyle=':', marker='o', markersize=10, \n",
    "            label=f'{columns[k]}', color=colors(k-6)\n",
    "        )\n",
    "        plt.title(f'Influence of Categorical and Binary Variables (Gower)')\n",
    "        plt.legend(title=\"Categorical and Binary Variables\", fontsize=10)\n",
    "        plt.title(\"Influence of Quantitative Variables (Gower)\", fontsize=12)\n",
    "        plt.xlabel('Coordinate 1')\n",
    "        plt.ylabel('Coordinate 2')\n",
    "        plt.grid()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# encode categorical/binaryvariables \n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "data_encoded = data_mds.copy()\n",
    "for col in categorical_variables:\n",
    "    data_encoded[col] = label_encoder.fit_transform(data[col])\n",
    "\n",
    "\n",
    "# run\n",
    "influence(X=data_encoded[numeric_variables+categorical_variables], p1= 7, p2 = 4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sensitivity of principal coordinates: \n",
    "- 1st via matlab scripts\n",
    "- 2nd via sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coorp_complet(D):\n",
    "    # This function needs to compute the principal coordinates of the full dataset\n",
    "    # (multidimensional scaling or classical MDS) and return the coordinates and cumulative percentage\n",
    "    pass\n",
    "\n",
    "def coorp_minus(Di):\n",
    "    # This function computes the principal coordinates after removing one individual\n",
    "    # (multidimensional scaling or classical MDS) and returns the coordinates and eigenvalues\n",
    "    pass\n",
    "\n",
    "def sensitividad_Gower(Xdat, p1, p2):\n",
    "    n, p = Xdat.shape\n",
    "    p3 = p - p1 - p2\n",
    "    \n",
    "    # Calculate the Gower similarity matrix\n",
    "    D = gower.gower_matrix(Xdat)\n",
    "    \n",
    "    # Principal coordinates for the full dataset\n",
    "    #Ycomplet, percentacum = coorp_complet(D)\n",
    "    Ycomplet, vaps, percentacum, acum = coorp(D)\n",
    "\n",
    "\n",
    "    # Leave-one-out sensitivity analysis\n",
    "    v = np.zeros((n, n))  # Initialize the matrix for squared distances\n",
    "    for i in range(n):\n",
    "        print(Xdat)\n",
    "        Xdati = np.delete(Xdat, i, axis=0)\n",
    "        Si = gower.gower_matrix(Xdati)\n",
    "        Di = np.ones((n - 1, n - 1)) - Si\n",
    "        \n",
    "        # Principal coordinates after removing the i-th individual\n",
    "        #Y, Lambda0 = coorp(Di)\n",
    "        Y, vaps, _, _ = coorp(Di)\n",
    "        v = 1.0 / vaps[:Y.shape[1]]\n",
    "        Lambda0 = np.diag(v)\n",
    "        g = np.diag(Y @ Y.T)\n",
    "        d = np.delete(D[:, i], i)  # Remove the i-th row and column\n",
    "        \n",
    "        # Interpolation of the i-th individual\n",
    "        y = 0.5 * (g - d) @ Y @ np.linalg.inv(Lambda0)\n",
    "        \n",
    "        # Insert the i-th individual back into the dataset\n",
    "        Yplusi = np.insert(Y, i, y, axis=0)\n",
    "        \n",
    "        # Calculate squared Euclidean distances\n",
    "        for j in range(n):\n",
    "            v[j, i] = np.sum((Yplusi[j, :] - Ycomplet[j, :]) ** 2)\n",
    "    \n",
    "    v = np.sqrt(v)\n",
    "    \n",
    "    # Hyper-sphere radii (quantiles)\n",
    "    radius = np.quantile(v, 0.95, axis=1)\n",
    "    \n",
    "    # Theta for circle plotting\n",
    "    theta = np.arange(0, 2 * np.pi, 0.12)\n",
    "    \n",
    "    # 2D graphical representation\n",
    "    plt.figure(1)\n",
    "    plt.plot(Ycomplet[:, 0], Ycomplet[:, 1], '.b')\n",
    "    \n",
    "    for i in range(n):\n",
    "        for j in range(len(theta)):\n",
    "            circle_x = Ycomplet[i, 0] + np.cos(theta[j]) * radius[i]\n",
    "            circle_y = Ycomplet[i, 1] + np.sin(theta[j]) * radius[i]\n",
    "            plt.plot(circle_x, circle_y, '.k', markersize=4)\n",
    "    \n",
    "    plt.title(f'Porcentaje de variabilidad explicada {percentacum}%', fontsize=12)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "sensitividad_Gower(data[numeric_variables+categorical_variables], p1= 7, p2 = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS TAKE A LONG TIME !!!\n",
    "from sklearn.manifold import MDS\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Function to preprocess the data (encode categorical columns)\n",
    "def preprocess_data(data):\n",
    "    # Apply Label Encoding to all categorical columns\n",
    "    le = LabelEncoder()\n",
    "    data_encoded = data.copy()\n",
    "    \n",
    "    for column in data_encoded.select_dtypes(include=['object']).columns:\n",
    "        data_encoded[column] = le.fit_transform(data_encoded[column])\n",
    "    \n",
    "    return data_encoded\n",
    "\n",
    "# Function to plot MDS after removing one point at a time\n",
    "def plot_mds(data):\n",
    "    # Preprocess data: convert categorical columns to numeric\n",
    "    data_encoded = preprocess_data(data)\n",
    "    \n",
    "    # Initialize the MDS object\n",
    "    mds = MDS(n_components=2, dissimilarity='euclidean', random_state=42)\n",
    "    \n",
    "    # Create a plot\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    # Perform MDS by removing one point at a time\n",
    "    for i in range(len(data_encoded)):\n",
    "        # Remove the i-th row (point) from the data\n",
    "        data_removed = data_encoded.drop(data_encoded.index[i])\n",
    "        \n",
    "        # Fit MDS to the remaining data\n",
    "        mds_fit = mds.fit_transform(data_removed)\n",
    "        \n",
    "        # Plot the first two principal coordinates (MD coordinates)\n",
    "        plt.scatter(mds_fit[:, 0], mds_fit[:, 1])\n",
    "    \n",
    "    # Customize plot\n",
    "    plt.title('MDS with One Point Removed Each Time')\n",
    "    plt.xlabel('MDS Component 1')\n",
    "    plt.ylabel('MDS Component 2')\n",
    "    plt.legend(loc='upper right', bbox_to_anchor=(1.2, 1))\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Example usage (assuming 'data' is your dataframe)\n",
    "# plot_mds(data)\n",
    "\n",
    "plot_mds(data[numeric_variables+categorical_variables].head(100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
